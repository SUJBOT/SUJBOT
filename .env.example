# ====================================================================
# MY_SUJBOT - Comprehensive Configuration
# ====================================================================
# Complete .env.example with ALL 128 parameters (from CONFIG_PARAMETERS_MAPPING.md)
# Last Updated: 2025-11-03
# See CLAUDE.md for detailed documentation
#
# PRIORITY MARKERS:
#   [REQUIRED]  - Must be set for basic functionality
#   [RECOMMENDED] - Should set for production
#   [OPTIONAL]  - Can leave as default
#   [ADVANCED]  - Only change if you know what you're doing
#   [IMMUTABLE] - Research-backed, DO NOT CHANGE

# ====================================================================
# SECTION 1: REQUIRED API KEYS
# ====================================================================
# At least ONE is required, depending on model selection

# [REQUIRED if LLM_MODEL starts with "claude-"]
# Anthropic Claude API key
ANTHROPIC_API_KEY=
# Get from: https://console.anthropic.com/

# [REQUIRED if LLM_MODEL starts with "gpt-", "o1-", "o3-" OR EMBEDDING_PROVIDER=openai]
# OpenAI API key (GPT models + embeddings)
OPENAI_API_KEY=
# Get from: https://platform.openai.com/api-keys

# [OPTIONAL if EMBEDDING_PROVIDER=voyage]
# Voyage AI API key
VOYAGE_API_KEY=
# Get from: https://www.voyageai.com/

# [OPTIONAL if LLM_MODEL starts with "gemini-"]
# Google Gemini API key
GOOGLE_API_KEY=
# Get from: https://aistudio.google.com/apikey

# ====================================================================
# SECTION 2: CORE MODEL SELECTION
# ====================================================================

# [RECOMMENDED] LLM model for summaries and agent (PHASE 2, PHASE 7)
# Claude: claude-sonnet-4-5-20250929, claude-haiku-4-5-20251001
# OpenAI GPT-4: gpt-4o, gpt-4o-mini
# OpenAI GPT-5: gpt-5, gpt-5-mini, gpt-5-nano
# Google Gemini: gemini-2.5-flash, gemini-2.5-pro
# Default: claude-sonnet-4-5-20250929 (best quality), gpt-4o-mini (best value)
LLM_MODEL=claude-sonnet-4-5-20250929

# [OPTIONAL] LLM provider override (auto-detected from LLM_MODEL if not set)
# Values: "claude", "openai", "google"
# Auto-detection: claude-* → "claude", gpt-* → "openai", gemini-* → "google"
# LLM_PROVIDER=claude

# [RECOMMENDED] Embedding model for vector store (PHASE 4)
# HuggingFace: bge-m3 (FREE, local, multilingual) ✅ RECOMMENDED
# OpenAI: text-embedding-3-large (3072D, highest quality)
# Voyage AI: voyage-3-large (SOTA MLEB benchmark)
# Default: text-embedding-3-large
EMBEDDING_MODEL=text-embedding-3-large

# [OPTIONAL] Embedding provider override (auto-detected from EMBEDDING_MODEL if not set)
# Values: "huggingface", "openai", "voyage"
# Auto-detection: bge-* → "huggingface", text-embedding-* → "openai", voyage-* → "voyage"
EMBEDDING_PROVIDER=openai

# ====================================================================
# SECTION 3: PHASE 1 - DOCUMENT EXTRACTION (Docling)
# ====================================================================

# [OPTIONAL] Enable OCR processing (default: true)
ENABLE_OCR=true

# [OPTIONAL] OCR engine: "tesseract" (Czech support) or "rapidocr" (3-5x faster)
# rapidocr requires: pip install rapidocr_onnxruntime
OCR_ENGINE=tesseract

# [OPTIONAL] OCR languages (comma-separated language codes, default: "ces,eng")
# Examples: "ces" (Czech), "eng" (English), "deu" (German), "fra" (French)
# Auto-detection: use "auto"
OCR_LANGUAGE=ces,eng

# [OPTIONAL] OCR recognition mode: "accurate" or "fast" (deprecated for Tesseract)
OCR_RECOGNITION=accurate

# [OPTIONAL] Extract tables from documents (default: true)
EXTRACT_TABLES=true

# [OPTIONAL] Table extraction mode: "ACCURATE" (recommended)
TABLE_MODE=ACCURATE

# [OPTIONAL] Extract document hierarchy (CRITICAL for hierarchical chunking, default: true)
# [IMMUTABLE] MUST be true for PHASE 3 hierarchical chunking to work
EXTRACT_HIERARCHY=true

# [OPTIONAL] Font-size based hierarchy detection (default: true)
# [IMMUTABLE] CRITICAL for hierarchical chunking - DO NOT DISABLE
ENABLE_SMART_HIERARCHY=true

# [OPTIONAL] BBox height clustering tolerance for hierarchy (pixels, default: 0.8)
# Lower = stricter clustering, 0.8 is research-optimal
HIERARCHY_TOLERANCE=0.8

# [OPTIONAL] Generate document/section summaries (default: true)
GENERATE_SUMMARIES=true

# [OPTIONAL] Override summary model (uses LLM_MODEL if not set)
# SUMMARY_MODEL=gpt-4o-mini

# [IMMUTABLE] Summary length target (characters, RCTS-optimal: DO NOT CHANGE)
SUMMARY_MAX_CHARS=150

# [IMMUTABLE] Summary style: "generic" (proven better than "expert", DO NOT CHANGE)
SUMMARY_STYLE=generic

# [OPTIONAL] Use OpenAI Batch API for summaries (default: true, 50% cheaper)
USE_BATCH_API=true

# [OPTIONAL] Seconds between batch status checks (default: 5)
BATCH_API_POLL_INTERVAL=5

# [OPTIONAL] Max wait for batch completion in seconds (default: 43200 = 12 hours)
BATCH_API_TIMEOUT=43200

# [OPTIONAL] Generate markdown output (default: true)
GENERATE_MARKDOWN=true

# [OPTIONAL] Generate JSON output (default: true)
GENERATE_JSON=true

# ====================================================================
# SECTION 3: PHASE 1 - DOCUMENT EXTRACTION (Unstructured.io)
# ====================================================================
# Supports multiple document formats:
# - PDF (.pdf) - with optional hi_res OCR models
# - PowerPoint (.pptx, .ppt) - presentation structure and text
# - Word (.docx, .doc) - document structure and formatting
# - HTML (.html, .htm) - web content with semantic structure
# - Plain text (.txt) - basic text files
# - LaTeX (.tex, .latex) - scientific documents

# [REQUIRED] Extraction strategy (default: hi_res)
# Options: "hi_res" (best quality), "fast", "ocr_only"
# NOTE: Only applies to PDF files. Other formats use format-specific strategies.
UNSTRUCTURED_STRATEGY=hi_res

# [RECOMMENDED] Hi-res model for PDF extraction (default: yolox - BEST FROM TESTING)
# Options:
#   - yolox: YOLOX model (best results from testing on legal documents, recommended)
#   - detectron2_mask_rcnn: Mask R-CNN X_101_32x8d_FPN_3x (alternative, slower)
#   - detectron2_onnx: Faster R-CNN R_50_FPN_3x (alternative, balanced)
#   - detectron2_quantized: Quantized model (alternative, fast)
# NOTE: Only applies to PDF files with hi_res strategy.
UNSTRUCTURED_MODEL=yolox

# [OPTIONAL] OCR languages (comma-separated, default: ces,eng)
# Supported: ces, eng, deu, fra, spa, ita, por, rus, pol, ukr, and more
# Applies to: PDF, PPTX, DOCX, TXT, LaTeX
UNSTRUCTURED_LANGUAGES=ces,eng

# [OPTIONAL] Per-element language detection (default: true)
# Detects language for each element individually (useful for multilingual docs)
# Applies to: All formats
UNSTRUCTURED_DETECT_LANGUAGE_PER_ELEMENT=true

# [OPTIONAL] Extract table structure (default: true)
# Applies to: PDF, PPTX, DOCX
UNSTRUCTURED_INFER_TABLE_STRUCTURE=true

# [OPTIONAL] Extract images from PDF (default: false)
# Applies to: PDF only
UNSTRUCTURED_EXTRACT_IMAGES=false

# [OPTIONAL] Include page breaks in output (default: true)
# Applies to: PDF, PPTX, DOCX (HTML has no pages)
UNSTRUCTURED_INCLUDE_PAGE_BREAKS=true

# ====================================================================
# HIERARCHY DETECTION (Generic, NOT language-specific)
# ====================================================================

# [OPTIONAL] Enable generic hierarchy detection (default: true)
ENABLE_GENERIC_HIERARCHY=true

# [OPTIONAL] Hierarchy detection signals (comma-separated)
# Options: type, font_size, spacing, indentation, numbering, length, parent_id
HIERARCHY_SIGNALS=type,font_size,spacing,numbering,parent_id

# [OPTIONAL] DBSCAN clustering epsilon (default: 0.15)
# Lower = more granular levels, Higher = fewer levels
HIERARCHY_CLUSTERING_EPS=0.15

# [OPTIONAL] Minimum samples per cluster (default: 2)
HIERARCHY_CLUSTERING_MIN_SAMPLES=2

# ====================================================================
# ROTATED TEXT FILTERING (Generic watermark detection)
# ====================================================================

# [OPTIONAL] Enable rotated text filtering (default: true)
FILTER_ROTATED_TEXT=true

# [OPTIONAL] Rotation method (default: bbox_orientation)
# Options: "bbox_orientation" (uses element bbox coordinates)
ROTATION_METHOD=bbox_orientation

# [OPTIONAL] Rotation angle thresholds in degrees (default: 25.0-65.0)
# Text rotated between these angles is filtered (diagonal watermarks)
ROTATION_MIN_ANGLE=25.0
ROTATION_MAX_ANGLE=65.0

# ====================================================================
# OUTPUT FORMATS
# ====================================================================

# [OPTIONAL] Generate markdown output (default: true)
GENERATE_MARKDOWN=true

# [OPTIONAL] Generate JSON output (default: true)
GENERATE_JSON=true

# ====================================================================
# SECTION 4: PHASE 2 - SUMMARIZATION
# ====================================================================

# [RECOMMENDED] Pipeline speed mode: "fast" (immediate) or "eco" (OpenAI Batch API, 50% cheaper)
# Controls use_batch_api behavior across PHASE 2, PHASE 3A
SPEED_MODE=fast

# [OPTIONAL] LLM temperature for summaries (default: 0.3, lower = consistent)
# Range: [0.0, 2.0], lower = more deterministic
SUMMARY_TEMPERATURE=0.3

# [OPTIONAL] Max output tokens for summary generation (default: 100)
# Optimized: 150 chars ≈ 40-60 tokens
SUMMARY_MAX_TOKENS=100

# [OPTIONAL] Retry if summary exceeds max_chars (default: true)
SUMMARY_RETRY_ON_EXCEED=true

# [OPTIONAL] Max retry attempts for summary generation (default: 3)
SUMMARY_MAX_RETRIES=3

# [OPTIONAL] Parallel summary generation threads (default: 20)
# [OPTIMIZED] Increased for 2x faster processing
SUMMARY_MAX_WORKERS=20

# [OPTIONAL] Minimum text length for summarization (default: 50 chars)
SUMMARY_MIN_TEXT_LENGTH=50

# [OPTIONAL] Enable prompt batching (default: false, disabled due to JSON overhead)
# Batch multiple sections in one API call (slower due to JSON marshaling)
SUMMARY_ENABLE_BATCHING=false

# [OPTIONAL] Number of sections per batch if batching enabled (default: 8)
SUMMARY_BATCH_SIZE=8

# [OPTIONAL] Use OpenAI Batch API (controlled by SPEED_MODE, default: true)
SUMMARY_USE_BATCH_API=true

# [OPTIONAL] Batch API poll interval in seconds (default: 5)
SUMMARY_BATCH_API_POLL_INTERVAL=5

# [OPTIONAL] Batch API timeout in seconds (default: 43200)
SUMMARY_BATCH_API_TIMEOUT=43200

# ====================================================================
# SECTION 5: PHASE 3A - CONTEXTUAL RETRIEVAL (LLM-based context generation)
# ====================================================================

# [RECOMMENDED] Enable contextual retrieval (default: true)
# Generates LLM-based context for each chunk
# -67% reduction in retrieval failures (Anthropic research)
ENABLE_CONTEXTUAL=true

# [OPTIONAL] Temperature for context generation (default: 0.3)
CONTEXT_GENERATION_TEMPERATURE=0.3

# [OPTIONAL] Max tokens for generated context (default: 150)
# Target: 50-100 words
CONTEXT_GENERATION_MAX_TOKENS=150

# [OPTIONAL] Include neighboring chunks for context (default: true)
CONTEXT_INCLUDE_SURROUNDING=true

# [OPTIONAL] Number of chunks before/after to include (default: 1)
CONTEXT_NUM_SURROUNDING_CHUNKS=1

# [OPTIONAL] Fallback to basic chunking if context generation fails (default: true)
CONTEXT_FALLBACK_TO_BASIC=true

# [OPTIONAL] Context generation batch size (default: 20)
# [OPTIMIZED] 2x faster processing
CONTEXT_BATCH_SIZE=20

# [OPTIONAL] Context generation parallel threads (default: 10)
CONTEXT_MAX_WORKERS=10

# [OPTIONAL] Use OpenAI Batch API for context generation (default: true, 50% cheaper)
CONTEXT_USE_BATCH_API=true

# [OPTIONAL] Batch API poll interval (default: 5)
CONTEXT_BATCH_API_POLL_INTERVAL=5

# [OPTIONAL] Batch API timeout in seconds (default: 43200)
CONTEXT_BATCH_API_TIMEOUT=43200

# ====================================================================
# SECTION 6: PHASE 3 - TOKEN-AWARE CHUNKING (HybridChunker)
# ====================================================================

# [IMMUTABLE] Max tokens per chunk (default: 512)
# Research-compatible: 512 tokens ≈ 500 chars for Czech/English mixed text
# LegalBench-RAG constraint preserved via token equivalence
# IMPORTANT: Must be <= embedding model max context (text-embedding-3-large: 8192)
MAX_TOKENS=512

# [RECOMMENDED] Tokenizer model (must match EMBEDDING_MODEL)
# OpenAI: text-embedding-3-large, text-embedding-3-small
# IMPORTANT: Uses tiktoken for OpenAI models
TOKENIZER_MODEL=text-embedding-3-large

# [RECOMMENDED] Enable Contextual Retrieval (SAC - Summary-Augmented Chunking)
# Reduces context drift by 58% (Reuter et al., 2024)
ENABLE_SAC=true

# REMOVED: CHUNK_SIZE, CHUNK_OVERLAP (RCTS legacy - replaced by HybridChunker)

# ====================================================================
# SECTION 7: PHASE 4 - EMBEDDING & FAISS INDEXING
# ====================================================================

# [OPTIONAL] Embedding batch size (default: 64)
# Larger = faster but more memory
EMBEDDING_BATCH_SIZE=64

# [OPTIONAL] Enable embedding cache (default: true, 40-80% hit rate)
EMBEDDING_CACHE_ENABLED=true

# [OPTIONAL] Max embedding cache entries (default: 1000)
EMBEDDING_CACHE_SIZE=1000

# [IMMUTABLE] Normalize embeddings for cosine similarity (always true for FAISS)
# DO NOT CHANGE - required for IndexFlatIP
NORMALIZE_EMBEDDINGS=true

# ====================================================================
# SECTION 8: PHASE 4.5 - SEMANTIC CLUSTERING (Optional)
# ====================================================================

# [OPTIONAL] Clustering algorithm: "hdbscan" (automatic, density-based) or "agglomerative" (hierarchical)
CLUSTERING_ALGORITHM=hdbscan

# [OPTIONAL] Number of clusters for agglomerative (ignored for HDBSCAN)
# Auto-detected if not set
# CLUSTERING_N_CLUSTERS=10

# [OPTIONAL] Minimum chunks per cluster for HDBSCAN (default: 5)
CLUSTERING_MIN_SIZE=5

# [OPTIONAL] Maximum clusters for auto-detection (agglomerative, default: 50)
CLUSTERING_MAX_CLUSTERS=50

# [OPTIONAL] Minimum clusters for auto-detection (agglomerative, default: 5)
CLUSTERING_MIN_CLUSTERS=5

# [OPTIONAL] Layers to cluster: "3" (chunks only) or "1,2,3" (all layers)
CLUSTERING_LAYERS=3

# [OPTIONAL] Generate semantic labels using LLM (default: true)
CLUSTERING_ENABLE_LABELS=true

# [OPTIONAL] Generate UMAP visualization (default: false)
CLUSTERING_ENABLE_VIZ=false

# [OPTIONAL] Visualization output directory (default: "output/clusters")
CLUSTERING_VIZ_DIR=output/clusters

# ====================================================================
# SECTION 9: PHASE 5 - ADVANCED RETRIEVAL
# ====================================================================

# [RECOMMENDED] Enable hybrid search (BM25 + Dense + RRF fusion)
# +23% precision improvement (Industry SOTA 2025)
ENABLE_HYBRID_SEARCH=true

# [OPTIONAL] RRF fusion parameter (default: 60, optimal for most use cases)
HYBRID_FUSION_K=60

# [RECOMMENDED] Enable knowledge graph extraction (PHASE 5A)
ENABLE_KNOWLEDGE_GRAPH=true

# [OPTIONAL] Minimum entity confidence threshold (default: 0.6, range: [0.0, 1.0])
KG_MIN_ENTITY_CONFIDENCE=0.6

# [OPTIONAL] Minimum relationship confidence threshold (default: 0.5, range: [0.0, 1.0])
KG_MIN_RELATIONSHIP_CONFIDENCE=0.5

# ====================================================================
# SECTION 10: PHASE 5A - KNOWLEDGE GRAPH BACKEND
# ====================================================================

# [RECOMMENDED] LLM provider for entity/relationship extraction (default: "openai")
# Values: "openai", "anthropic"
KG_LLM_PROVIDER=openai

# [RECOMMENDED] LLM model for KG extraction (default: "gpt-4o-mini")
# [OPTIMIZED] 70% cheaper than gpt-5-mini ($0.15/$0.60 vs $0.50/$2.00)
KG_LLM_MODEL=gpt-4o-mini

# [RECOMMENDED] Graph storage backend (default: "simple")
# Values: "simple" (JSON, development), "neo4j" (production, ALL TOOLS), "networkx" (in-memory)
# NOTE: Set to "neo4j" for production deployments + CLI WebApp compatibility
KG_BACKEND=simple

# [OPTIONAL] JSON export path for simple backend
KG_EXPORT_PATH=./output/graphs/knowledge_graph.json

# [OPTIONAL] Enable verbose logging for KG extraction (default: true)
KG_VERBOSE=true

# [OPTIONAL] Enable entity extraction (default: true)
ENABLE_ENTITY_EXTRACTION=true

# [OPTIONAL] Enable relationship extraction (default: true)
ENABLE_RELATIONSHIP_EXTRACTION=true

# [OPTIONAL] Extract cross-document relationships (default: false, expensive)
ENABLE_CROSS_DOCUMENT_RELATIONSHIPS=false

# [OPTIONAL] Max retry attempts for KG extraction (default: 3)
KG_MAX_RETRIES=3

# [OPTIONAL] Delay between retries in seconds (default: 1.0)
KG_RETRY_DELAY=1.0

# [OPTIONAL] Timeout per extraction batch in seconds (default: 300)
KG_TIMEOUT=300

# [OPTIONAL] KG extraction log file path (default: "./logs/kg_extraction.log")
KG_LOG_PATH=./logs/kg_extraction.log

# -------- ENTITY EXTRACTION CONFIGURATION --------

# [OPTIONAL] LLM provider for entity extraction (default: "openai")
ENTITY_EXTRACTION_LLM_PROVIDER=openai

# [OPTIONAL] LLM model for entity extraction (default: "gpt-4o-mini")
ENTITY_EXTRACTION_LLM_MODEL=gpt-4o-mini

# [OPTIONAL] Temperature for deterministic extraction (default: 0.0, must be 0.0)
ENTITY_EXTRACTION_TEMPERATURE=0.0

# [OPTIONAL] Minimum confidence threshold for entities (default: 0.6, range: [0.0, 1.0])
ENTITY_EXTRACTION_MIN_CONFIDENCE=0.6

# [OPTIONAL] Extract entity definitions (default: true)
ENTITY_EXTRACTION_EXTRACT_DEFINITIONS=true

# [OPTIONAL] Normalize entity values (default: true)
ENTITY_EXTRACTION_NORMALIZE=true

# [OPTIONAL] Batch size for entity extraction (default: 20)
# [OPTIMIZED] 2x faster processing
ENTITY_EXTRACTION_BATCH_SIZE=20

# [OPTIONAL] Parallel extraction threads (default: 10)
ENTITY_EXTRACTION_MAX_WORKERS=10

# [OPTIONAL] Cache extraction results (default: true)
ENTITY_EXTRACTION_CACHE_RESULTS=true

# [OPTIONAL] Include few-shot examples in prompt (default: true)
ENTITY_EXTRACTION_INCLUDE_EXAMPLES=true

# [OPTIONAL] Max entities per chunk (default: 50)
ENTITY_EXTRACTION_MAX_PER_CHUNK=50

# [OPTIONAL] Enabled entity types (comma-separated, default: all 30 types)
# Core: STANDARD,ORGANIZATION,DATE,CLAUSE,TOPIC,PERSON,LOCATION,CONTRACT
# Regulatory: REGULATION,DECREE,DIRECTIVE,TREATY,LEGAL_PROVISION,REQUIREMENT
# Authorization: PERMIT,LICENSE_CONDITION
# Nuclear: REACTOR,FACILITY,SYSTEM,SAFETY_FUNCTION,FUEL_TYPE,ISOTOPE
# Events: INCIDENT,EMERGENCY_CLASSIFICATION,INSPECTION,DECOMMISSIONING_PHASE
# Liability: LIABILITY_REGIME
# ENTITY_EXTRACTION_ENABLED_TYPES=STANDARD,ORGANIZATION,DATE,CLAUSE,TOPIC,PERSON,LOCATION,CONTRACT

# -------- RELATIONSHIP EXTRACTION CONFIGURATION --------

# [OPTIONAL] LLM provider for relationship extraction (default: "openai")
RELATIONSHIP_EXTRACTION_LLM_PROVIDER=openai

# [OPTIONAL] LLM model for relationship extraction (default: "gpt-4o-mini")
RELATIONSHIP_EXTRACTION_LLM_MODEL=gpt-4o-mini

# [OPTIONAL] Temperature for deterministic extraction (default: 0.0)
RELATIONSHIP_EXTRACTION_TEMPERATURE=0.0

# [OPTIONAL] Minimum confidence threshold for relationships (default: 0.5, range: [0.0, 1.0])
RELATIONSHIP_EXTRACTION_MIN_CONFIDENCE=0.5

# [OPTIONAL] Extract supporting evidence text (default: true)
RELATIONSHIP_EXTRACTION_EXTRACT_EVIDENCE=true

# [OPTIONAL] Max characters for evidence snippets (default: 200)
RELATIONSHIP_EXTRACTION_MAX_EVIDENCE_LENGTH=200

# [OPTIONAL] Extract relationships within single chunk (default: true)
RELATIONSHIP_EXTRACTION_WITHIN_CHUNK=true

# [OPTIONAL] Extract relationships across chunks (default: true)
RELATIONSHIP_EXTRACTION_CROSS_CHUNK=true

# [OPTIONAL] Extract relationships from chunk metadata (default: true)
RELATIONSHIP_EXTRACTION_FROM_METADATA=true

# [OPTIONAL] Batch size for relationship extraction (default: 10)
# [OPTIMIZED] 2x faster processing
RELATIONSHIP_EXTRACTION_BATCH_SIZE=10

# [OPTIONAL] Parallel extraction threads (default: 10)
RELATIONSHIP_EXTRACTION_MAX_WORKERS=10

# [OPTIONAL] Cache extraction results (default: true)
RELATIONSHIP_EXTRACTION_CACHE_RESULTS=true

# [OPTIONAL] Max relationships per entity (default: 100)
RELATIONSHIP_EXTRACTION_MAX_PER_ENTITY=100

# [OPTIONAL] Enabled relationship types (comma-separated, default: all 40 types)
# Compliance: COMPLIES_WITH,CONTRADICTS,PARTIALLY_SATISFIES,SPECIFIES_REQUIREMENT,REQUIRES_CLAUSE
# Regulatory: IMPLEMENTS,TRANSPOSES,SUPERSEDED_BY,SUPERSEDES,AMENDS
# Structure: CONTAINS_CLAUSE,CONTAINS_PROVISION,CONTAINS,PART_OF
# Citations: REFERENCES,REFERENCED_BY,CITES_PROVISION,BASED_ON
# Authorization: ISSUED_BY,GRANTED_BY,ENFORCED_BY,SUBJECT_TO_INSPECTION,SUPERVISES
# Technical: REGULATED_BY,OPERATED_BY,HAS_SYSTEM,PERFORMS_FUNCTION,USES_FUEL,CONTAINS_ISOTOPE
# Temporal: EFFECTIVE_DATE,EXPIRY_DATE,SIGNED_ON,DECOMMISSIONED_ON
# Content: COVERS_TOPIC,APPLIES_TO
# Provenance: MENTIONED_IN,DEFINED_IN,DOCUMENTED_IN
# RELATIONSHIP_EXTRACTION_ENABLED_TYPES=COMPLIES_WITH,CONTRADICTS,REFERENCES,CONTAINS,PART_OF

# -------- NEO4J CONFIGURATION (Optional, for production) --------

# [RECOMMENDED] Neo4j connection URI
# Local: bolt://localhost:7687
# Neo4j Aura: neo4j+s://YOUR_INSTANCE_ID.databases.neo4j.io
NEO4J_URI=neo4j+s://YOUR_INSTANCE_ID.databases.neo4j.io

# [OPTIONAL] Neo4j username (default: "neo4j")
NEO4J_USERNAME=neo4j

# [CRITICAL] Neo4j password (from Aura setup, cannot be recovered!)
NEO4J_PASSWORD=YOUR_AURA_PASSWORD

# [OPTIONAL] Neo4j database name (default: "neo4j")
NEO4J_DATABASE=neo4j

# [OPTIONAL] Max connection lifetime in seconds (default: 3600)
NEO4J_MAX_CONNECTION_LIFETIME=3600

# [OPTIONAL] Max concurrent connections (default: 50)
NEO4J_MAX_CONNECTION_POOL_SIZE=50

# [OPTIONAL] Connection timeout in seconds (default: 30)
NEO4J_CONNECTION_TIMEOUT=30

# [OPTIONAL] Auto-create indexes for entity types (default: true)
NEO4J_CREATE_INDEXES=true

# [OPTIONAL] Auto-create uniqueness constraints (default: true)
NEO4J_CREATE_CONSTRAINTS=true

# -------- ENTITY DEDUPLICATION CONFIGURATION --------

# [RECOMMENDED] Master switch for entity deduplication (default: true)
# Prevents duplicate entities when indexing multiple documents into Neo4j
# 3-layer strategy: Exact match → Semantic similarity → Acronym expansion
KG_DEDUPLICATE_ENTITIES=true

# [OPTIONAL] Layer 2: Use embedding-based semantic similarity (default: false)
# Latency: 50-200ms per entity
# Requires embeddings, may have false positives
KG_DEDUP_USE_EMBEDDINGS=false

# [OPTIONAL] Cosine similarity threshold for Layer 2 (default: 0.90, range: [0.0, 1.0])
KG_DEDUP_SIMILARITY_THRESHOLD=0.90

# [OPTIONAL] Layer 3: Enable acronym expansion (default: false)
# Domain-specific + fuzzy matching, 100-500ms latency
# Recommended for production: Layer 1 + Layer 3 (98% precision, 85% recall)
KG_DEDUP_USE_ACRONYM_EXPANSION=false

# [OPTIONAL] Fuzzy match threshold for Layer 3 (default: 0.85, range: [0.0, 1.0])
KG_DEDUP_ACRONYM_FUZZY_THRESHOLD=0.85

# [OPTIONAL] Custom acronym mappings (format: "ACRO1:expansion1,ACRO2:expansion2")
# Examples: "ISO:International Organization for Standardization"
# KG_DEDUP_CUSTOM_ACRONYMS=

# [OPTIONAL] Use APOC for deduplication, fallback to Cypher (default: true)
# APOC ~10-20ms per 1000 entities, pure Cypher ~20-50ms
KG_DEDUP_APOC_ENABLED=true

# [OPTIONAL] Embedding model for Layer 2 deduplication (default: "text-embedding-3-large")
KG_DEDUP_EMBEDDING_MODEL=text-embedding-3-large

# [OPTIONAL] Batch size for Layer 2 embeddings (default: 100)
KG_DEDUP_EMBEDDING_BATCH_SIZE=100

# [OPTIONAL] Cache embeddings for Layer 2 (default: true)
KG_DEDUP_CACHE_EMBEDDINGS=true

# [OPTIONAL] Create Neo4j uniqueness constraints (default: true)
KG_DEDUP_CREATE_CONSTRAINTS=true

# -------- GRAPH STORAGE CONFIGURATION --------

# [OPTIONAL] Graph storage backend (default: "simple")
# Values: "simple" (JSON), "neo4j", "networkx"
GRAPH_STORAGE_BACKEND=simple

# [OPTIONAL] JSON store path for simple backend (default: "./data/graphs/simple_graph.json")
GRAPH_SIMPLE_STORE_PATH=./data/graphs/simple_graph.json

# [OPTIONAL] Export to JSON after construction (default: true)
GRAPH_EXPORT_JSON=true

# [OPTIONAL] JSON export path (default: "./data/graphs/knowledge_graph.json")
GRAPH_EXPORT_PATH=./data/graphs/knowledge_graph.json

# [OPTIONAL] Deduplicate entities during graph construction (default: true)
GRAPH_DEDUPLICATE_ENTITIES=true

# [OPTIONAL] Merge similar entities (default: false, expensive)
GRAPH_MERGE_SIMILAR_ENTITIES=false

# [OPTIONAL] Similarity threshold for entity merging (default: 0.9, range: [0.0, 1.0])
GRAPH_SIMILARITY_THRESHOLD=0.9

# [OPTIONAL] Track provenance (chunk sources) for entities (default: true)
GRAPH_TRACK_PROVENANCE=true

# ====================================================================
# SECTION 11: PHASE 6 - CONTEXT ASSEMBLY
# ====================================================================
# No configuration parameters for this phase (internal processing)

# ====================================================================
# SECTION 12: PHASE 7 - RAG AGENT (Interactive CLI/API)
# ====================================================================

# [RECOMMENDED] Agent model (can differ from summary model)
# Default: claude-haiku-4-5 (fastest, cheapest)
# Better: claude-sonnet-4-5 (higher quality for complex queries)
# Cost-optimized: gpt-4o-mini
AGENT_MODEL=claude-haiku-4-5

# [OPTIONAL] Max output tokens for agent responses (default: 4096)
# Gemini: up to 8192
# GPT-5: up to 16384+
# Claude: up to 4096
# AGENT_MAX_TOKENS=4096

# [OPTIONAL] Agent temperature (default: 0.3)
# Range: [0.0, 2.0], lower = more deterministic
AGENT_TEMPERATURE=0.3

# [OPTIONAL] Enable tool validation on startup (default: true)
AGENT_ENABLE_TOOL_VALIDATION=true

# [OPTIONAL] Enable debug mode (default: false)
AGENT_DEBUG_MODE=false

# [REQUIRED] Vector store path (must point to phase4_vector_store directory)
VECTOR_STORE_PATH=vector_db

# [OPTIONAL] Knowledge graph path (if using KG with agent)
KNOWLEDGE_GRAPH_PATH=output/knowledge_graph.json

# [RECOMMENDED] Enable prompt caching (Anthropic only)
# 90% cost reduction on cached tokens (system prompt, tools, init messages)
# Default: true
ENABLE_PROMPT_CACHING=true

# [RECOMMENDED] Enable context management (prevents quadratic cost growth)
# Auto-prunes old tool results in long conversations
# Default: true
ENABLE_CONTEXT_MANAGEMENT=true

# [OPTIONAL] Token threshold for context pruning (default: 50000)
# Starts pruning when input tokens exceed this
CONTEXT_MANAGEMENT_TRIGGER=50000

# [OPTIONAL] Keep last N messages with full tool context (default: 3)
CONTEXT_MANAGEMENT_KEEP=3

# [RECOMMENDED] Query expansion LLM model
# Default: gpt-4o-mini (stable, fast)
# Alternatives: gpt-5-nano (experimental), claude-haiku-4-5
QUERY_EXPANSION_MODEL=gpt-4o-mini

# -------- AGENT TOOL CONFIGURATION --------

# [OPTIONAL] Default number of results to retrieve (default: 6, range: [1, 100])
TOOL_DEFAULT_K=6

# [RECOMMENDED] Enable cross-encoder reranking (default: true)
# +25% accuracy improvement (LegalBench-RAG)
TOOL_ENABLE_RERANKING=true

# [OPTIONAL] Number of candidates before reranking (default: 50)
# Must be >= TOOL_DEFAULT_K
TOOL_RERANKER_CANDIDATES=50

# [OPTIONAL] Reranker model (default: "bge-reranker-large")
# Options: "bge-reranker-large" (SOTA), "bge-reranker-base", "ms-marco-mini"
TOOL_RERANKER_MODEL=bge-reranker-large

# [OPTIONAL] Enable graph-based result boosting (default: true)
# +200-500ms overhead, +8% factual correctness on entity queries
TOOL_ENABLE_GRAPH_BOOST=true

# [OPTIONAL] Weight for graph boosting (default: 0.3, range: [0.0, 1.0])
TOOL_GRAPH_BOOST_WEIGHT=0.3

# [OPTIONAL] Max documents to compare (default: 3)
TOOL_MAX_DOCUMENT_COMPARE=3

# [OPTIONAL] Legal compliance confidence threshold (default: 0.7)
# HIGH: >= 0.85, MEDIUM: 0.70-0.84, LOW: 0.50-0.69
TOOL_COMPLIANCE_THRESHOLD=0.7

# [OPTIONAL] Context window for expansion (chunks before/after, default: 2)
TOOL_CONTEXT_WINDOW=2

# [OPTIONAL] Lazy load reranker (default: false)
# Set true to speed up agent startup (load reranker on first search)
TOOL_LAZY_LOAD_RERANKER=false

# [OPTIONAL] Lazy load knowledge graph (default: true)
# Set false to load graph on startup (slower startup, faster first query)
TOOL_LAZY_LOAD_GRAPH=true

# [OPTIONAL] Cache embeddings (default: true)
TOOL_CACHE_EMBEDDINGS=true

# ====================================================================
# SECTION 13: CLI CONFIGURATION
# ====================================================================

# [OPTIONAL] Display citations in responses (default: true)
CLI_SHOW_CITATIONS=true

# [OPTIONAL] Citation format: "inline", "footnote", "detailed", "simple" (default: "inline")
CLI_CITATION_FORMAT=inline

# [OPTIONAL] Display tool calls in output (default: true)
CLI_SHOW_TOOL_CALLS=true

# [OPTIONAL] Display timing information (default: true)
CLI_SHOW_TIMING=true

# [OPTIONAL] Enable streaming responses (default: true)
CLI_ENABLE_STREAMING=true

# [OPTIONAL] Save conversation history (default: true)
CLI_SAVE_HISTORY=true

# [OPTIONAL] History file path (default: ".agent_history")
CLI_HISTORY_FILE=.agent_history

# [OPTIONAL] Max history items to keep (default: 1000)
CLI_MAX_HISTORY_ITEMS=1000

# ====================================================================
# SECTION 14: PIPELINE CONFIGURATION
# ====================================================================

# [OPTIONAL] Logging level (default: "INFO")
# Options: "DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"
LOG_LEVEL=INFO

# [OPTIONAL] Log file path (default: "logs/pipeline.log")
LOG_FILE=logs/pipeline.log

# [OPTIONAL] Data directory for input documents (default: "data")
DATA_DIR=data

# [OPTIONAL] Output directory for pipeline results (default: "output")
OUTPUT_DIR=output

# ====================================================================
# SECTION 15: RESEARCH CONSTRAINTS - DO NOT CHANGE
# ====================================================================
# These parameters are backed by SOTA research and should NOT be modified
# without extensive testing and peer review.
#
# CHUNK_SIZE=500
#   - Optimal value from LegalBench-RAG (Pipitone & Alami, 2024)
#   - Proven optimal for legal/technical documents
#   - Changing breaks all retrieval quality metrics
#
# SUMMARY_STYLE=generic
#   - Proven better than "expert" summaries (Reuter et al., 2024)
#   - Counterintuitive but rigorously tested
#   - "Expert" summaries perform worse on retrieval
#
# CHUNK_OVERLAP=0
#   - RCTS (Recursive Character Text Splitter) handles naturally
#   - Overlap not needed for hierarchical architecture
#
# NORMALIZE_EMBEDDINGS=true
#   - Required for FAISS IndexFlatIP (cosine similarity)
#   - Changing breaks vector store
#
# ENABLE_SMART_HIERARCHY=true
#   - Critical for multi-layer hierarchical chunking
#   - Font-size classification is essential for document structure
#
# ====================================================================
# SECTION 16: ADVANCED / INTERNAL PARAMETERS
# ====================================================================
# These parameters are for advanced users and internal tuning.
# Most users should NOT modify these.

# [ADVANCED] LLM provider override for KG extraction (auto-detected from KG_LLM_MODEL)
# KG_LLM_PROVIDER_OVERRIDE=openai

# [ADVANCED] Document structure depth for hierarchy (typically 3-4 levels)
# Higher = more granular, but may fragment semantically related content
# HIERARCHY_DEPTH=4

# [ADVANCED] Minimum section length for separate hierarchy level
# Shorter sections may not warrant separate level
# MIN_SECTION_LENGTH=100

# ====================================================================
# NOTES FOR CONFIGURATION
# ====================================================================
#
# 1. API KEY PRIORITY:
#    - Set ANTHROPIC_API_KEY for Claude models
#    - Set OPENAI_API_KEY for GPT models + OpenAI embeddings
#    - Set VOYAGE_API_KEY for Voyage embeddings
#    - Set GOOGLE_API_KEY for Gemini models
#
# 2. MODEL SELECTION STRATEGY:
#    Cost-optimized: gpt-4o-mini + text-embedding-3-large
#    Quality-optimized: claude-sonnet + voyage-3-large
#    Free development: gemini-2.5-flash + bge-m3
#
# 3. PRODUCTION SETUP:
#    - Set KG_BACKEND=neo4j for production deployments
#    - Enable ENABLE_PROMPT_CACHING for cost savings
#    - Set SPEED_MODE=eco for bulk indexing
#    - Use claude-sonnet-4-5 or gpt-4o for agent
#
# 4. RESEARCH PARAMETERS:
#    - CHUNK_SIZE: 500 (DO NOT CHANGE - research optimal)
#    - SUMMARY_STYLE: generic (DO NOT CHANGE - proven better)
#    - ENABLE_SMART_HIERARCHY: true (CRITICAL for hierarchical chunking)
#
# 5. PERFORMANCE TUNING:
#    - Increase *_MAX_WORKERS for faster processing
#    - Increase *_BATCH_SIZE for memory efficiency
#    - Set SPEED_MODE=eco to use cheaper Batch API
#
# 6. TROUBLESHOOTING:
#    - If embeddings fail: Check EMBEDDING_MODEL + EMBEDDING_PROVIDER match
#    - If agent fails: Check VECTOR_STORE_PATH points to valid phase4_vector_store
#    - If KG tools unavailable: Set KG_BACKEND=neo4j
#    - If cache not working: Set EMBEDDING_CACHE_ENABLED=true
#
# ====================================================================

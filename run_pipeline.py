#!/usr/bin/env python3
"""
Complete RAG Pipeline Runner - FULL SOTA 2025 SYSTEM

Runs complete indexing pipeline with ALL advanced features:
- PHASE 1: Smart hierarchy extraction (Docling)
- PHASE 2: Generic summaries
- PHASE 3: Multi-layer chunking + SAC (contextual retrieval)
- PHASE 4: Embeddings + FAISS indexing
- PHASE 5A: Knowledge Graph extraction ‚úÖ
- PHASE 5B: Hybrid Search (BM25 + Dense + RRF) ‚úÖ

Supported formats: PDF, DOCX, PPTX, XLSX, HTML
Input: Single document OR directory (batch processing)

Usage:
    # Single document
    python run_pipeline.py <document_path>
    python run_pipeline.py "data/document.pdf"

    # Batch processing (directory)
    python run_pipeline.py <directory_path>
    python run_pipeline.py "data/documents/"

Outputs saved to: output/<document_name>/<timestamp>/
- phase1_extraction.json - Document structure & hierarchy
- phase2_summaries.json - Generic summaries
- phase3_chunks.json - Multi-layer chunks with SAC
- phase4_vector_store/ - FAISS + BM25 indexes + metadata
- <document_id>_kg.json - Knowledge graph (if enabled)

Configuration: All settings controlled via config.json file
- Copy config.json.example to config.json and fill in your values
- See config.json.example for available options
- Hybrid Search: "hybrid_search": {"enable": true}
- Knowledge Graph: "knowledge_graph": {"enable": true}
"""

import sys
import logging
import argparse
import shutil
from pathlib import Path

# CRITICAL: Apply nest_asyncio early to allow nested event loops
# Required because PostgreSQL adapter uses asyncio.run() internally
import nest_asyncio
nest_asyncio.apply()

# CRITICAL: Validate config.json before doing anything else
try:
    from src.config import get_config
    _config = get_config()  # This will fail if config.json is invalid or missing
except FileNotFoundError as e:
    print(f"\n‚ùå ERROR: config.json not found!")
    print(f"\nPlease create config.json from config.json.example:")
    print(f"  cp config.json.example config.json")
    print(f"  # Edit config.json with your settings")
    sys.exit(1)
except ValueError as e:
    print(f"\n‚ùå ERROR: Invalid configuration in config.json!")
    print(f"\n{e}")
    print(f"\nPlease fix the errors in config.json")
    print(f"See config.json.example for reference")
    sys.exit(1)
except Exception as e:
    print(f"\n‚ùå ERROR: Failed to load configuration!")
    print(f"\n{e}")
    sys.exit(1)

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

from src.indexing_pipeline import IndexingPipeline, IndexingConfig
from src.cost_tracker import get_global_tracker, reset_global_tracker

# LlamaIndex wrapper with state persistence and entity labeling (optional)
try:
    from src.indexing import SujbotIngestionPipeline
    LLAMAINDEX_WRAPPER_AVAILABLE = True
except ImportError:
    LLAMAINDEX_WRAPPER_AVAILABLE = False


def print_header(text: str):
    """Print formatted header."""
    print()
    print("=" * 80)
    print(text)
    print("=" * 80)
    print()


def print_success(text: str):
    """Print success message."""
    print(f"[OK]{text}")


def print_info(text: str):
    """Print info message."""
    print(f"  {text}")


def get_supported_documents(directory: Path) -> list:
    """
    Get list of all supported documents in directory.

    Args:
        directory: Path to directory

    Returns:
        List of document paths
    """
    supported_formats = [".pdf", ".docx", ".pptx", ".xlsx", ".html", ".htm", ".txt", ".tex", ".latex"]
    documents = []

    for ext in supported_formats:
        documents.extend(directory.glob(f"*{ext}"))
        documents.extend(directory.glob(f"*{ext.upper()}"))

    # Remove duplicates and sort
    documents = sorted(set(documents))

    return documents


def run_complete_pipeline(
    input_path: Path,
    output_base: Path = None,
    merge_target: Path = None,
    storage_backend: str = None,
    use_wrapper: bool = None,
):
    """
    Run complete SOTA 2025 RAG pipeline.

    Features:
    - Smart hierarchy extraction
    - Generic summaries
    - Contextual retrieval (SAC)
    - Hybrid search (BM25 + Dense + RRF)
    - Knowledge graph extraction
    - Automatic merge with existing vector store
    - Entity labeling (when using LlamaIndex wrapper)

    Args:
        input_path: Path to document file or directory
        output_base: Base output directory (default: output/)
        merge_target: Path to existing vector store to merge into (e.g., vector_db/)
        storage_backend: Storage backend override ('faiss' or 'postgresql', default: from config.json)
        use_wrapper: Use LlamaIndex wrapper for state persistence and entity labeling
    """
    input_path = Path(input_path)

    if not input_path.exists():
        print(f"‚úó Error: Path not found: {input_path}")
        sys.exit(1)

    # Check if it's a directory
    if input_path.is_dir():
        documents = get_supported_documents(input_path)
        if not documents:
            print(f"‚úó Error: No supported documents found in directory: {input_path}")
            print(f"  Supported formats: PDF, DOCX, PPTX, XLSX, HTML")
            sys.exit(1)

        print_header(f"BATCH PROCESSING - {len(documents)} documents from {input_path.name}")
        print_info(f"Found documents:")
        for doc in documents:
            print_info(f"  - {doc.name}")
        print()

        # Process each document
        for i, document_path in enumerate(documents, 1):
            print()
            print("=" * 80)
            print(f"PROCESSING [{i}/{len(documents)}]: {document_path.name}")
            print("=" * 80)
            print()
            run_single_document(document_path, output_base, merge_target, storage_backend, use_wrapper)

        print_header("BATCH PROCESSING COMPLETE")
        print_success(f"Processed {len(documents)} documents")
        print_info(f"Outputs saved to: {output_base or Path('output')}")
        return

    # Single document processing
    run_single_document(input_path, output_base, merge_target, storage_backend, use_wrapper)


def run_single_document(
    document_path: Path,
    output_base: Path = None,
    merge_target: Path = None,
    storage_backend: str = None,
    use_wrapper: bool = None,
):
    """
    Process single document through complete SOTA 2025 pipeline.

    Args:
        document_path: Path to document file
        output_base: Base output directory (default: output/)
        merge_target: Path to existing vector store to merge into (e.g., vector_db/)
        storage_backend: Storage backend override ('faiss' or 'postgresql', default: from config.json)
        use_wrapper: Use LlamaIndex wrapper for state persistence and entity labeling
    """
    document_path = Path(document_path)

    # Validate format
    supported_formats = [".pdf", ".docx", ".pptx", ".xlsx", ".html", ".htm", ".txt", ".tex", ".latex"]
    if document_path.suffix.lower() not in supported_formats:
        print(f"‚úó Error: Unsupported format: {document_path.suffix}")
        print(f"  Supported formats: {', '.join(supported_formats)}")
        return

    # Setup output directory
    if output_base is None:
        output_base = Path(__file__).parent / "output"

    doc_name = document_path.stem.replace(" ", "_").replace("(", "").replace(")", "")
    output_dir = output_base / doc_name  # No timestamp - enables resume functionality
    output_dir.mkdir(parents=True, exist_ok=True)

    print_header(f"SOTA 2025 RAG PIPELINE - {document_path.name}")

    # Reset cost tracker for this document
    reset_global_tracker()

    # Create pipeline with FULL SOTA configuration from .env
    print_info("Initializing pipeline with SOTA 2025 configuration...")

    # Create config with optional storage backend override
    config_overrides = {}
    if storage_backend is not None:
        config_overrides["storage_backend"] = storage_backend

    config = IndexingConfig.from_env(**config_overrides)  # Loads all settings from config.json

    # Determine whether to use LlamaIndex wrapper
    # Priority: CLI flag > config.json > default (False)
    if use_wrapper is None:
        # Check config.json for indexing.use_llamaindex_wrapper
        root_config = get_config()
        use_wrapper = getattr(root_config.indexing, 'use_llamaindex_wrapper', False)

    # Print active configuration
    print_info(f"LLM Model: {config.summarization_config.model}")
    print_info(f"Embedding Model: {config.embedding_config.model}")
    print_info(f"Max Tokens: {config.chunking_config.max_tokens} tokens (HybridChunker)")
    print_info(f"SAC (Contextual Retrieval): {'ON' if config.chunking_config.enable_contextual else 'OFF'}")
    print_info(f"HyDE + Expansion Fusion: ON (w_hyde=0.6, w_exp=0.4)")
    print_info(f"Knowledge Graph: {'ON' if config.enable_knowledge_graph else 'OFF'}")
    print_info(f"Storage Backend: {config.storage_backend.upper()}")

    # Show wrapper/entity labeling status
    if use_wrapper and LLAMAINDEX_WRAPPER_AVAILABLE:
        print_info(f"LlamaIndex Wrapper: ON (state persistence + entity labeling)")
        entity_model = root_config.indexing.entity_labeling_model  # From RootConfig.indexing
        print_info(f"Entity Labeling: ON ({entity_model})")
    elif use_wrapper and not LLAMAINDEX_WRAPPER_AVAILABLE:
        print_info(f"LlamaIndex Wrapper: REQUESTED but dependencies not installed")
        use_wrapper = False
    else:
        print_info(f"LlamaIndex Wrapper: OFF (use --use-wrapper to enable)")

    print_info(f"Output: {output_dir}")
    print()

    # Create pipeline (wrapper or legacy)
    if use_wrapper and LLAMAINDEX_WRAPPER_AVAILABLE:
        root_config = get_config()
        pipeline = SujbotIngestionPipeline(
            config=config,
            enable_entity_labeling=root_config.indexing.enable_entity_labeling,
            entity_labeling_batch_size=root_config.indexing.entity_labeling_batch_size,
            entity_labeling_model=root_config.indexing.entity_labeling_model,
        )
    else:
        pipeline = IndexingPipeline(config)

    # Run pipeline with intermediate saves
    try:
        print_header("RUNNING PIPELINE (ALL PHASES)")

        result = pipeline.index_document(
            document_path=document_path,
            save_intermediate=True,  # Save phase1-3 JSON outputs
            output_dir=output_dir
        )

        # Check if indexing was skipped due to duplicate detection
        if result is None:
            print_header("DOCUMENT SKIPPED")
            print_info("Document was identified as duplicate and skipped")
            print_info("No indexing or merging performed")
            return

        vector_store = result["vector_store"]
        knowledge_graph = result["knowledge_graph"]
        stats = result["stats"]

        # PHASE 4: Vector store is stored in PostgreSQL (no file save needed)
        print_success(f"Vector store persisted in PostgreSQL database")

        # Save PHASE 5A: Knowledge Graph (if enabled)
        # Note: KG may already be saved by indexing_pipeline.py
        kg_path = None
        if knowledge_graph:
            kg_path = output_dir / f"{doc_name}_kg.json"
            if kg_path.exists():
                print_success(f"Knowledge graph saved: {kg_path}")
            else:
                # Fallback: save using appropriate method
                print_info("KG not saved by indexing pipeline, using fallback save...")
                try:
                    import json
                    # Try save_json method first (KnowledgeGraph), fallback to to_dict (GraphitiExtractionResult)
                    if hasattr(knowledge_graph, 'save_json'):
                        knowledge_graph.save_json(str(kg_path))
                    else:
                        with open(kg_path, "w", encoding="utf-8") as f:
                            json.dump(knowledge_graph.to_dict(), f, indent=2, ensure_ascii=False)
                    print_success(f"Knowledge graph saved (via fallback): {kg_path}")
                except (IOError, PermissionError) as e:
                    print_info(f"[WARNING] Could not save knowledge graph to {kg_path}: {e}")

        # Knowledge graph merging with --merge flag (PostgreSQL handles vector merging automatically)
        if merge_target:
            merge_target = Path(merge_target)
            print()
            print_info(f"Note: PostgreSQL stores vectors directly in database (no manual merging needed)")

            # Knowledge graph merging
            if knowledge_graph and config.enable_knowledge_graph:
                # Skip merge for GraphitiExtractionResult - Graphiti stores entities directly in Neo4j
                from src.graph.graphiti_extractor import GraphitiExtractionResult
                if isinstance(knowledge_graph, GraphitiExtractionResult):
                    print()
                    print_info(f"Graphiti KG stored directly in Neo4j: {knowledge_graph.total_entities} entities, "
                              f"{knowledge_graph.total_relationships} relationships")
                    print_info("Skipping unified KG merge (Graphiti uses Neo4j as primary storage)")
                else:
                    try:
                        print()
                        print_info("Merging knowledge graphs with cross-document deduplication...")

                        from src.graph import (
                            KnowledgeGraph,
                            UnifiedKnowledgeGraphManager,
                            CrossDocumentRelationshipDetector
                        )

                        # Initialize unified KG manager (uses merge_target directory for storage)
                        manager = UnifiedKnowledgeGraphManager(storage_dir=str(merge_target))

                        # Initialize cross-document detector
                        detector = CrossDocumentRelationshipDetector(
                            use_llm_validation=False,  # Fast pattern-based detection
                            confidence_threshold=0.7
                        )

                        # Load or create unified KG
                        unified_kg = manager.load_or_create()

                        print_info(f"Current unified KG: {len(unified_kg.entities)} entities, "
                                  f"{len(unified_kg.relationships)} relationships")
                        print_info(f"New document KG: {len(knowledge_graph.entities)} entities, "
                                  f"{len(knowledge_graph.relationships)} relationships")

                        # Merge with deduplication and cross-doc detection
                        unified_kg = manager.merge_document_graph(
                            unified_kg=unified_kg,
                            document_kg=knowledge_graph,
                            document_id=doc_name,
                            cross_doc_detector=detector
                        )

                        # Save unified KG + per-document backup
                        manager.save(unified_kg, document_id=doc_name)

                        # Get statistics
                        doc_stats = manager.get_document_statistics(unified_kg)

                        print_success(f"KG merge complete with cross-document relationships!")
                        print_info(f"Unified KG: {len(unified_kg.entities)} entities, "
                                  f"{len(unified_kg.relationships)} relationships")
                        print_info(f"Documents in unified KG: {doc_stats['total_documents']}")
                        print_info(f"Cross-document entities: {doc_stats['cross_document_entities']} "
                                  f"({doc_stats['cross_document_entity_percentage']:.1f}%)")
                        print_info(f"Saved: {merge_target / 'unified_kg.json'}")

                    except FileNotFoundError as e:
                        print()
                        print_info(f"[ERROR] KG file not found: {e}")
                        logger.error(f"KG merge failed - file missing: {e}", exc_info=True)
                        logger.error(f"Document: {doc_name}, Merge target: {merge_target}")
                        print_info(f"Document '{doc_name}' KG will not be merged into unified graph")
                    except PermissionError as e:
                        print()
                        print_info(f"[ERROR] Cannot write to {merge_target}: Permission denied")
                        logger.error(f"KG merge failed - permission error: {e}", exc_info=True)
                        print_info(f"Document '{doc_name}' KG will not be merged")
                    except (KeyError, AttributeError) as e:
                        # Data structure errors - likely data corruption
                        print()
                        print_info(f"[ERROR] KG data structure error: {e}")
                        logger.error(f"KG merge failed - data integrity issue: {e}", exc_info=True)
                        logger.error(f"Document: {doc_name}")
                        if 'unified_kg' in locals():
                            logger.error(f"Unified KG state: {len(unified_kg.entities)} entities, {len(unified_kg.relationships)} relationships")
                        if hasattr(knowledge_graph, 'entities'):
                            logger.error(f"New KG state: {len(knowledge_graph.entities)} entities, {len(knowledge_graph.relationships)} relationships")
                        print_info(f"Document '{doc_name}' KG appears corrupted - skipping merge")
                    except (ValueError, RuntimeError) as e:
                        # Expected validation/merge errors
                        print()
                        print_info(f"[ERROR] KG merge validation failed: {e}")
                        logger.error(f"KG merge failed - validation error: {e}", exc_info=True)
                        print_info(f"Document '{doc_name}' KG validation failed - skipping merge")
                    # Removed broad Exception catch - let unexpected errors propagate!

        # Print comprehensive statistics
        print_header("INDEXING COMPLETE")
        print_success(f"Document processed: {document_path.name}")
        print()

        print_info("üìä PIPELINE STATISTICS")
        print_info("-" * 60)

        # Chunking stats
        chunking = stats["chunking"]
        print_info(f"Layer 1 (documents):  {chunking['layer1_count']:4d} chunks")
        print_info(f"Layer 2 (sections):   {chunking['layer2_count']:4d} chunks")
        print_info(f"Layer 3 (paragraphs): {chunking['layer3_count']:4d} chunks (PRIMARY)")
        print_info(f"Total chunks:         {chunking['total_chunks']:4d}")

        if 'layer3_avg_size' in chunking:
            print()
            print_info(f"Layer 3 avg size:     {chunking['layer3_avg_size']:.0f} chars")
            if 'context_avg_overhead' in chunking:
                print_info(f"SAC context overhead: {chunking['context_avg_overhead']:.0f} chars/chunk")

        # Vector store stats
        vs_stats = stats["vector_store"]
        print()
        print_info(f"Vector dimensions:    {vs_stats.get('dense_dimensions', vs_stats.get('dimensions', 'N/A'))}D")
        print_info(f"Total vectors:        {vs_stats['total_vectors']}")
        print_info(f"Documents indexed:    {vs_stats['documents']}")

        # Retrieval method info
        print()
        print_info("‚úÖ Retrieval Method: HyDE + Expansion Fusion")
        print_info(f"   HyDE weight:       0.6")
        print_info(f"   Expansion weight:  0.4")
        print_info(f"   Storage:           PostgreSQL pgvector")

        # Knowledge graph stats
        if stats.get("kg_construction_failed"):
            print()
            print_info("[FAILED]Knowledge Graph: FAILED")
            print_info(f"   Error: {stats.get('kg_error', 'Unknown error')}")
            print_info("   Continuing with vector search only")
        elif stats.get("kg_enabled") and knowledge_graph:
            print()
            print_info("‚úÖ Knowledge Graph: ACTIVE")
            print_info(f"   Entities:          {stats['kg_entities']}")
            print_info(f"   Relationships:     {stats['kg_relationships']}")
        elif stats.get("kg_enabled"):
            print()
            print_info("[WARNING]  Knowledge Graph: ENABLED but no graph generated")
        else:
            print()
            print_info("[INFO]  Knowledge Graph: DISABLED")

        print()

    except FileNotFoundError as e:
        logger.error(f"Document not found: {e}", exc_info=True)
        print(f"\n‚úó ERROR: Document not found")
        print(f"   {e}")
        print("\nPlease check:")
        print(f"  - Path is correct: {document_path}")
        print("  - File exists and is readable")
        sys.exit(1)

    except PermissionError as e:
        logger.error(f"Permission denied: {e}", exc_info=True)
        print(f"\n‚úó ERROR: Permission denied")
        print(f"   {e}")
        print("\nPlease check:")
        print("  - File/directory permissions")
        print("  - Current user has read/write access")
        sys.exit(1)

    except MemoryError as e:
        logger.error(f"Out of memory: {e}", exc_info=True)
        print(f"\n‚úó ERROR: Out of memory")
        print(f"   Document may be too large: {document_path}")
        print("\nSolutions:")
        print("  - Process smaller documents")
        print("  - Increase system memory")
        print("  - Reduce batch_size in config")
        sys.exit(1)

    except KeyboardInterrupt:
        logger.info("Pipeline interrupted by user")
        print("\n\n[WARNING]  Pipeline interrupted by user (Ctrl+C)")
        sys.exit(130)

    except Exception as e:
        # Check if it's an API error (works for both anthropic and openai)
        error_str = str(e).lower()
        if any(keyword in error_str for keyword in ['api', 'authentication', 'auth', 'key', 'quota', 'rate limit']):
            logger.error(f"API error: {e}", exc_info=True)
            print(f"\n‚úó ERROR: API call failed")
            print(f"   {e}")
            print("\nPlease check:")
            print("  - API keys are valid (check .env file)")
            print("  - API quota/rate limits")
            print("  - Internet connectivity")
            print("  - API service status")
        else:
            logger.error(f"Pipeline failed: {e}", exc_info=True)
            print(f"\n‚úó ERROR: {e}")
            print()
            print("Common issues:")
            print("  - Missing API keys (check .env file)")
            print("  - Invalid document format")
            print("  - Insufficient disk space")
            print("\nFor more details, check the log file above.")
        sys.exit(1)

    # Print cost summary
    tracker = get_global_tracker()

    if tracker.total_cost > 0:
        print_header("üí∞ API COST SUMMARY")
        print_info(f"Total cost:        ${tracker.total_cost:.4f} USD")
        print_info(f"Total calls:       {len(tracker.entries)}")
        print_info(f"Input tokens:      {tracker.total_input_tokens:,}")
        print_info(f"Output tokens:     {tracker.total_output_tokens:,}")
        print()

        # Cost by provider
        if tracker.cost_by_provider:
            print_info("Cost by provider:")
            for provider, cost in tracker.cost_by_provider.items():
                print_info(f"  {provider:12s}  ${cost:.4f}")
            print()

        # Cost by operation
        if tracker.cost_by_operation:
            print_info("Cost by operation:")
            for operation, cost in tracker.cost_by_operation.items():
                print_info(f"  {operation:12s}  ${cost:.4f}")
        print()

    # Final success message
    print("=" * 80)
    print(f"‚úÖ ALL DONE! Outputs saved in: {output_dir}")
    print("=" * 80)
    print()
    print("üìÅ Output files:")
    print(f"  ‚Ä¢ phase1_extraction.json    - Document structure")
    print(f"  ‚Ä¢ phase2_summaries.json      - Section summaries")
    print(f"  ‚Ä¢ phase3_chunks.json         - Multi-layer chunks")
    if storage_backend != "postgresql":
        print(f"  ‚Ä¢ phase4_vector_store/       - FAISS + BM25 indexes")
    if knowledge_graph:
        print(f"  ‚Ä¢ {doc_name}_kg.json         - Knowledge graph")
    print()
    print("üöÄ To use with agent:")
    if storage_backend == "postgresql":
        print("  uv run python -m src.agent.cli  # (uses PostgreSQL backend)")
    else:
        vs_path = output_dir / "phase4_vector_store"
        print(f"  uv run python -m src.agent.cli --vector-store {vs_path}")
    print()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Complete SOTA 2025 RAG Pipeline - Index documents with automatic merge support",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Index single document (auto-merges to vector_db/)
  python run_pipeline.py data/document.pdf

  # Index directory batch (auto-merges all to vector_db/)
  python run_pipeline.py data/documents/

  # Index to custom location instead of vector_db
  python run_pipeline.py data/document.pdf --merge custom_db

  # Index without merging (keep separate)
  python run_pipeline.py data/document.pdf --no-merge
        """
    )

    parser.add_argument(
        "input_path",
        type=str,
        help="Path to document file or directory to index"
    )

    parser.add_argument(
        "--merge",
        type=str,
        metavar="TARGET",
        default="vector_db",  # Default to vector_db/ for automatic merging
        help="Merge indexed documents into existing vector store at TARGET path (default: vector_db)"
    )

    parser.add_argument(
        "--no-merge",
        action="store_true",
        help="Disable automatic merging to vector_db (keep documents separate)"
    )

    parser.add_argument(
        "--backend",
        type=str,
        choices=["faiss", "postgresql"],
        default=None,
        help="Storage backend for vectors (default: from config.json). Options: faiss, postgresql"
    )

    parser.add_argument(
        "--use-wrapper",
        action="store_true",
        default=None,
        help="Use LlamaIndex wrapper for state persistence and entity labeling (requires Redis)"
    )

    parser.add_argument(
        "--no-wrapper",
        action="store_true",
        help="Disable LlamaIndex wrapper (override config.json setting)"
    )

    args = parser.parse_args()

    # Handle wrapper flags
    use_wrapper = None
    if args.use_wrapper:
        use_wrapper = True
    elif args.no_wrapper:
        use_wrapper = False

    input_path = Path(args.input_path)
    # Use merge target unless --no-merge is specified
    merge_target = None if args.no_merge else Path(args.merge)

    run_complete_pipeline(
        input_path,
        merge_target=merge_target,
        storage_backend=args.backend,
        use_wrapper=use_wrapper,
    )

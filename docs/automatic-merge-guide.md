# Automatic Merge & Knowledge Graph Integration Guide

**Created:** 2025-10-29
**Status:** âœ… Complete

## Overview

This guide documents the complete automatic merge system that allows seamless addition of new documents to your existing vector database with intelligent deduplication and Knowledge Graph integration.

## Features

### 1. Automatic Vector Store Merging

The system automatically merges new documents into your existing vector database with these capabilities:

- **FAISS Index Merging** - All 3 layers (L1, L2, L3) merged automatically
- **BM25 Index Merging** - Vocabularies combined and indexes rebuilt
- **Knowledge Graph Merging** - Entity deduplication with relationship ID remapping
- **Duplicate Detection** - Semantic similarity check prevents re-indexing duplicates
- **Graceful Creation** - Creates new vector_db/ if it doesn't exist

### 2. Shell Script Integration

The `add_to_vector_db.sh` script has been simplified to use a single command:

```bash
uv run python run_pipeline.py "$INPUT_PATH" --merge vector_db
```

This handles all merge operations automatically.

## Usage

### Basic Usage

```bash
# Add a single document
./add_to_vector_db.sh /path/to/document.pdf

# Add a directory of documents
./add_to_vector_db.sh /path/to/documents/
```

### Manual Python Usage

```bash
# Index and merge a single document
uv run python run_pipeline.py data/document.pdf --merge vector_db

# Index and merge a directory
uv run python run_pipeline.py data/regulations/ --merge vector_db

# Just index without merging (output to document-specific directory)
uv run python run_pipeline.py data/document.pdf
```

## How It Works

### Merge Workflow

When you run with `--merge vector_db`:

1. **Document Indexing** (Phases 1-5)
   - Extract hierarchy (Phase 1)
   - Generate summaries (Phase 2)
   - Create multi-layer chunks with SAC (Phase 3)
   - Generate embeddings and FAISS indexes (Phase 4)
   - Build Knowledge Graph if enabled (Phase 5A)
   - Create BM25 indexes (Phase 5B)

2. **Duplicate Detection** (Before Merge)
   - Extract first page text with PyMuPDF
   - Generate embedding for first page
   - Compare with existing documents (98% similarity threshold)
   - Skip merge if duplicate detected

3. **Automatic Merge** (If Not Duplicate)

   **FAISS Merge:**
   ```python
   # Merge all 3 layers
   existing_store.faiss_store.merge(new_store.faiss_store)
   ```

   **BM25 Merge:**
   ```python
   # Combine vocabularies and rebuild
   existing_store.bm25_store.merge(new_store.bm25_store)
   ```

   **Knowledge Graph Merge:**
   ```python
   # Load existing KG
   existing_kg = SimpleGraphBuilder.load_json(existing_kg_path)

   # Merge with entity deduplication
   merge_stats = existing_pipeline.merge_graphs(new_pipeline)

   # Entity deduplication uses (type, normalized_value) tuples
   # Relationship IDs automatically remapped
   ```

4. **Save Merged Store**
   ```python
   # Save to vector_db/
   existing_store.save(merge_target)
   ```

### Entity Deduplication

The Knowledge Graph merge includes intelligent entity deduplication:

**Deduplication Key:** `(entity_type, normalized_value)`

**Example:**
```python
# These are considered duplicates:
Entity(type="regulation", value="ZÃ¡kon 18/1997 Sb.")
Entity(type="regulation", value="zÃ¡kon 18/1997 sb.")  # Same after normalization
Entity(type="regulation", value="ZÃ¡kon Ä. 18/1997 Sb.")

# After merge: Only 1 entity remains with all relationships preserved
```

**Merge Statistics:**
```python
{
    'entities_added': 45,      # New unique entities
    'entities_deduplicated': 12,  # Duplicates merged
    'relationships_added': 89,    # New relationships
    'relationships_remapped': 31  # IDs updated for merged entities
}
```

## Configuration

### Environment Variables (.env)

```bash
# Enable/disable features
ENABLE_KNOWLEDGE_GRAPH=true          # Build KG during indexing
ENABLE_DUPLICATE_DETECTION=true      # Skip duplicate documents

# Knowledge Graph LLM
KG_LLM_PROVIDER=openai
KG_LLM_MODEL=gpt-4o-mini            # Recommended for reliable JSON

# Note: gpt-5-nano has poor JSON quality - use gpt-4o-mini instead
```

### Model Recommendations

**For Knowledge Graph Extraction:**

âœ… **Recommended:** `gpt-4o-mini`
- Fast and cost-effective
- Reliable JSON generation
- No parsing errors

âŒ **Not Recommended:** `gpt-5-nano`
- Poor JSON output quality
- ~70% parsing error rate in testing
- Slower due to retries

**GPT-5 API Compatibility:**

GPT-5 models require different API parameters:

```python
# GPT-4 style (old):
params = {
    "max_tokens": 4000,
    "temperature": 0.7
}

# GPT-5 style (new):
params = {
    "max_completion_tokens": 4000,
    # temperature: only 1.0 supported (don't set)
}
```

Our code automatically handles this in:
- `src/graph/entity_extractor.py`
- `src/graph/relationship_extractor.py`

## File Structure

### After Adding Documents

```
vector_db/
â”œâ”€â”€ faiss_layer1.index           # Layer 1 FAISS index (merged)
â”œâ”€â”€ faiss_layer2.index           # Layer 2 FAISS index (merged)
â”œâ”€â”€ faiss_layer3.index           # Layer 3 FAISS index (merged)
â”œâ”€â”€ faiss_arrays.pkl             # FAISS data arrays (merged)
â”œâ”€â”€ faiss_metadata.json          # FAISS configuration
â”œâ”€â”€ bm25_layer1_arrays.pkl       # BM25 Layer 1 (rebuilt after merge)
â”œâ”€â”€ bm25_layer2_arrays.pkl       # BM25 Layer 2 (rebuilt after merge)
â”œâ”€â”€ bm25_layer3_arrays.pkl       # BM25 Layer 3 (rebuilt after merge)
â”œâ”€â”€ bm25_layer*_config.json      # BM25 configurations
â”œâ”€â”€ hybrid_config.json           # Hybrid search config
â”œâ”€â”€ document1_kg.json            # Knowledge Graph for document 1
â”œâ”€â”€ document2_kg.json            # Knowledge Graph for document 2
â””â”€â”€ ...                          # More KG files (one per document)
```

### Individual Document Outputs

```
output/
â””â”€â”€ document_name/
    â””â”€â”€ timestamp/
        â”œâ”€â”€ phase1_hierarchy.json      # Extracted structure
        â”œâ”€â”€ phase2_summaries.json      # Generic summaries
        â”œâ”€â”€ phase3_chunks.json         # Multi-layer chunks with SAC
        â”œâ”€â”€ phase4_vector_store/       # Pre-merge vector store
        â”‚   â”œâ”€â”€ faiss_layer*.index
        â”‚   â”œâ”€â”€ bm25_layer*.pkl
        â”‚   â””â”€â”€ ...
        â””â”€â”€ phase5a_knowledge_graph.json  # Pre-merge KG (if enabled)
```

## Creating Knowledge Graph for Existing Documents

If you have documents already indexed but without Knowledge Graphs:

### Method 1: Re-index with KG Enabled

```bash
# Make sure KG is enabled
echo "ENABLE_KNOWLEDGE_GRAPH=true" >> .env

# Re-index the document (will be skipped as duplicate, but KG will be created)
# Actually, this won't work due to duplicate detection

# Disable duplicate detection temporarily
sed -i 's/ENABLE_DUPLICATE_DETECTION=true/ENABLE_DUPLICATE_DETECTION=false/' .env

# Re-index with KG
uv run python run_pipeline.py /path/to/document.pdf --merge vector_db

# Re-enable duplicate detection
sed -i 's/ENABLE_DUPLICATE_DETECTION=false/ENABLE_DUPLICATE_DETECTION=true/' .env
```

### Method 2: Build KG from Phase3 Chunks (Manual)

```python
import json
from pathlib import Path
from src.graph import KnowledgeGraphPipeline, KnowledgeGraphConfig

# Load configuration
kg_config = KnowledgeGraphConfig.from_env()

# Load phase3 chunks
phase3_file = Path("output/document_name/timestamp/phase3_chunks.json")
with open(phase3_file, 'r', encoding='utf-8') as f:
    data = json.load(f)

# Prepare chunks (Layer 2 + Layer 3)
chunks = []
for layer_key in ["layer2", "layer3"]:
    for chunk in data.get(layer_key, []):
        chunks.append({
            "id": chunk["chunk_id"],
            "content": chunk["content"],
            "metadata": chunk.get("metadata", {})
        })

# Build Knowledge Graph
document_id = "document_name"
with KnowledgeGraphPipeline(kg_config) as pipeline:
    kg = pipeline.build_from_chunks(chunks, document_id=document_id)

    # Save to vector_db
    kg_path = Path("vector_db") / f"{document_id}_kg.json"
    kg.save_json(str(kg_path))

    print(f"âœ… KG created: {len(kg.entities)} entities, {len(kg.relationships)} relationships")
```

## Troubleshooting

### Issue: "Document already exists in vector_db"

**Cause:** Trying to add a duplicate document

**Solution:** This is expected behavior - the document is already indexed. If you want to update the KG, see "Creating Knowledge Graph for Existing Documents" above.

### Issue: GPT-5 JSON parsing errors

**Error:** `Failed to parse LLM response as JSON: Expecting value: line 1 column 1 (char 0)`

**Cause:** GPT-5-nano has poor JSON generation quality

**Solution:** Switch to gpt-4o-mini in .env:
```bash
sed -i 's/KG_LLM_MODEL=gpt-5-nano/KG_LLM_MODEL=gpt-4o-mini/' .env
```

### Issue: FAISS merge fails

**Error:** `ValueError: Cannot merge indexes with different dimensions`

**Cause:** Using different embedding models for different documents

**Solution:** Always use the same embedding model (EMBEDDING_MODEL in .env):
```bash
# macOS M1/M2/M3
EMBEDDING_MODEL=bge-m3

# Windows or cloud preference
EMBEDDING_MODEL=text-embedding-3-large
```

### Issue: Knowledge Graph not being created

**Symptoms:** Pipeline shows "Knowledge Graph: OFF"

**Causes:**
1. ENABLE_KNOWLEDGE_GRAPH not set to true
2. Inline comments in .env blocking parsing

**Solution:**
```bash
# Check current setting
grep ENABLE_KNOWLEDGE_GRAPH .env

# Fix if needed (remove any comments)
sed -i 's/ENABLE_KNOWLEDGE_GRAPH=true # .*/ENABLE_KNOWLEDGE_GRAPH=true/' .env
```

## Performance Notes

### Merge Speed

**FAISS Merge:** ~100-500ms depending on document count
- O(1) operation for adding vectors
- No re-indexing required (IndexFlatIP)

**BM25 Merge:** ~200-1000ms depending on vocabulary size
- O(n) vocabulary combination
- Requires full index rebuild

**Knowledge Graph Merge:** ~50-200ms depending on entity count
- O(n) entity deduplication using normalized value index
- O(m) relationship ID remapping

### Memory Usage

**During Merge:**
- Peak memory: ~2-3x the size of existing vector store
- Temporary: New document embeddings held in memory
- After merge: Returns to normal (~1x vector store size)

**Optimization Tips:**
1. Merge documents in batches rather than one-by-one
2. Use eco mode for overnight bulk processing
3. Monitor memory with large document collections (>1000 docs)

## Research-Based Design Decisions

### Why Generic Summaries?

âœ… **Generic summaries** (150 chars) reduce DRM (Document Retrieval Misalignment) by **58%** compared to expert summaries

**Reference:** Reuter et al. (2024) - Summary-Augmented Chunking

### Why 3 Separate FAISS Indexes?

âœ… **Multi-layer indexing** retrieves **2.3x more essential chunks** compared to single-layer

**Reference:** Lima (2024) - Multi-Layer Embeddings

### Why Entity Deduplication?

âœ… **Entity deduplication** prevents Knowledge Graph bloat and improves relationship accuracy

**Example:** Without deduplication, merging 5 documents about the same regulation would create 5 separate entities. With deduplication, only 1 entity exists with all relationships combined.

## Code References

### Core Implementation Files

- `run_pipeline.py:233-346` - Automatic merge logic
- `src/faiss_vector_store.py:merge()` - FAISS merge implementation
- `src/hybrid_search.py:merge()` - BM25 merge implementation
- `src/graph/graph_builder.py:merge_graphs()` - KG merge with deduplication
- `src/graph/deduplicator.py` - Entity deduplication logic
- `src/duplicate_detector.py` - Semantic duplicate detection

### Testing

```bash
# Test automatic merge
uv run pytest tests/graph/test_graph_merge.py -v

# Test entity deduplication
uv run pytest tests/graph/test_deduplicator.py -v

# Test duplicate detection
uv run pytest tests/test_duplicate_detector.py -v
```

## Summary

The automatic merge system provides:

âœ… **Zero-friction document addition** - Just run the script
âœ… **Intelligent deduplication** - Both semantic (documents) and entity-level (KG)
âœ… **Automatic merging** - FAISS, BM25, and Knowledge Graph all merged seamlessly
âœ… **Research-based design** - Following proven best practices from 4 papers
âœ… **Production-ready** - Graceful error handling and comprehensive logging

**Usage:**
```bash
./add_to_vector_db.sh /path/to/your/document.pdf
```

That's it! ðŸš€

---

**For More Information:**
- Main README: `/README.md`
- Pipeline Documentation: `/PIPELINE.md`
- Agent Documentation: `/docs/agent/README.md`
- Cost Tracking: `/docs/cost-tracking.md`

# Cost-optimized legal knowledge graph extraction can cut expenses by 90% while improving accuracy

Recent 2024-2025 research demonstrates that **hybrid approaches combining prompt optimization, intelligent routing, and caching can reduce LLM-based knowledge graph extraction costs by 70-95%** while maintaining or exceeding GPT-4 baseline accuracy. For legal documents specifically, the combination of automated prompt optimization (GEPA), hybrid model routing, and incremental graph construction achieves the most dramatic cost reductions—**with systems like gpt-oss-120b outperforming Claude Opus 4.1 by ~3% while costing 90x less**. The key insight: individual optimization techniques compound multiplicatively rather than additively, making 90%+ total cost reductions achievable for production legal KG systems processing 200+ page documents.

Your current implementation using gpt-4o-mini with parallel processing (batch_size=20, max_workers=10) and per-chunk LLM calls provides a solid foundation but leaves substantial optimization opportunities. Modern approaches can reduce your costs by 50-80% through prompt compression, strategic caching, and hybrid extraction methods while simultaneously improving extraction accuracy through better entity resolution and relationship extraction techniques.

## Dramatic cost reductions through prompt optimization and model routing

The most impactful cost optimization comes from **automated prompt optimization combined with intelligent model routing**, validated in production systems throughout 2024-2025. Databricks' GEPA (Gradient-free Evolutionary Prompt Algorithm) achieved remarkable results: open-source gpt-oss-120b surpassed Claude Opus 4.1 by approximately 3% while being 90x cheaper. When applied to Claude Sonnet 4, GEPA improved performance by 6-7%. At scale (100k requests), optimization overhead becomes negligible; at 10M requests, it's essentially free. Compared to supervised fine-tuning, GEPA delivers comparable performance while reducing serving costs by 20%.

**Prompt compression** represents another breakthrough optimization. Microsoft's LLMLingua-2 (ACL 2024) achieves 3x-6x faster processing than its predecessor with superior generalization, compressing prompts by up to 20x with minimal performance degradation. On the GSM8K benchmark, LLMLingua maintained 98.5% accuracy at 20x compression. For legal documents, LongLLMLingua specifically addresses "lost in the middle" problems in long contexts, achieving 21.4% improvement in RAG systems while using only 25% of tokens. The implementation uses coarse-to-fine compression with small language models (GPT-2-small, LLaMA-7B), featuring budget controllers for semantic integrity and token-level iterative compression.

**Hybrid model routing** provides 40% cost reduction by assigning queries to appropriately-sized models based on difficulty. The ICLR 2024 "Hybrid LLM" paper demonstrated that a DeBERTa-v3-large (300M parameters) router can direct simple extractions to Llama-2-13b, which outperforms GPT-3.5-turbo on ~20% of information extraction examples. For legal contracts, this means routing standard clause extraction to smaller models while reserving expensive models for ambiguous or complex provisions. The ARTER framework (2025) takes this further for entity linking, achieving 95.82% reduction in LLM calls with only -1.61% accuracy loss by using frozen retrieval models (ReFinED) for easy cases and LLM reasoning only for hard cases.

**Model selection fundamentally impacts costs**. Comparing options for legal extraction: Amazon NOVA Micro performs structured extraction within 2% of GPT-4 accuracy while being 285x cheaper. GPT-4o-mini ($0.15 per 1M input tokens, $0.60 per 1M output tokens) offers 60% cost reduction versus GPT-3.5 Turbo with "significantly better" structured extraction performance. Small models (7B-13B parameters) like Llama-2-13b or Mistral-7b match GPT-3.5 quality at 1/100th cost for structured data extraction tasks when properly prompted. The KG-RAG study demonstrated that Llama-2-13b achieves comparable accuracy to GPT-4 at 90x lower cost for knowledge graph construction from legal documents.

For your implementation, these optimizations compound dramatically. Starting with GPT-4o-mini (already cost-effective), adding LLMLingua-2 compression (20x), implementing hybrid routing (40% reduction on remaining costs), and strategic caching (30-50% additional savings) yields overall cost reductions of 75-90% compared to baseline GPT-4 approaches. The specific combination for legal documents: use GPT-4o-mini with prompt compression for entity extraction, route relationship extraction through a difficulty classifier (small model for explicit relationships, larger model for implicit ones), and cache results for repeated entity types and standard legal clauses.

## Intelligent caching strategies deliver 30-65% token savings for legal documents

**Semantic caching** has emerged as a critical optimization for legal document processing where standard clauses and entity types repeat frequently. Research from 2025 (arXiv 2504.02268) confirms that approximately 33% of LLM queries exhibit semantic similarity to previous queries, matching web search patterns. Unlike exact-match caching, semantic caching uses embedding models to compute query similarity, triggering cache hits when cosine similarity exceeds a threshold. For legal documents with standardized provisions (force majeure clauses, indemnification, governing law), domain-specific embeddings fine-tuned for one epoch show significant improvement. **OpenAI offers 75% discounts for cached inputs** with GPT-4.1 nano, making this optimization immediately cost-effective.

**KV caching** provides 5x or greater speedup for long sequence generation by caching key-value pairs in transformer attention layers. This transforms quadratic-time operations into linear time, enabling scaling to longer contexts (8K, 32K tokens) essential for 200+ page legal documents. Combined with response caching for identical queries, total token savings reach 30% or more for repeated query handling.

The **KG-RAG optimization framework** (Nature Communications PMC11441322, 2024) demonstrated 53.9% token reduction compared to Cypher-RAG and 65.1% versus full-text indexing for knowledge graph construction. The method uses minimal graph schema for context extraction plus embedding-based pruning, maintaining 97% retrieval accuracy under query perturbation with only 3,693 average tokens versus 8,006 (Cypher-RAG) or 10,590 (full-text). The framework defines upper limits on graph connections (context volume hyperparameter), achieving 50% token reduction without accuracy compromise by extracting only semantically relevant context.

Implementation for your system: Deploy three-layer caching with (1) semantic caching for entity type queries using Legal-BGE embeddings with 0.85+ cosine similarity threshold, (2) KV caching for long document processing, and (3) response caching for extracted standard clauses. Conservative estimates suggest 40-50% cumulative token savings for legal contract processing. The architecture stores cached embeddings in Redis, checks semantic similarity before each LLM call, and invalidates cache entries based on document version updates. For repeated entity mentions within single documents, implement local context caching where entity descriptions are computed once and referenced throughout chunk processing.

## Hybrid extraction combining rules, smaller models, and targeted LLM use

**Hybrid architectures** strategically combine rule-based systems, small specialized models, and large LLMs to optimize cost-accuracy tradeoffs. The fundamental principle: route deterministic extractions to rules, straightforward patterns to small models, and reserve expensive LLMs for ambiguous cases requiring reasoning. Width.ai's legal document pipeline (2024) exemplifies this approach with two stages: (1) custom OCR with text processing on GPU, (2) entity extraction with confidence intervals and false entity filtering using surrounding keyword information. The system achieves **90%+ accuracy with 3.5 seconds average runtime** on a single GPU.

The "Replace Extraction" method (arXiv 2510.10138) improves efficiency for repetitive legal documents by replacing structured elements with unique placeholders. This enables prompt reuse across similar contracts, reducing token consumption and improving consistency. For legal domains with standard contract templates (employment agreements, NDAs, standard service contracts), this approach dramatically reduces per-document extraction costs while maintaining high accuracy on variable fields.

**Template-based extraction** for standard legal clauses achieves 97.71% average accuracy according to 2024 DORA compliance studies. The LlamaExtract + Neo4j pipeline demonstrates production implementation: LlamaParse handles PDF parsing, LLM-based classification determines contract type (Affiliate Agreements, Co-Branding, Development), then schema-based extraction using Pydantic models extracts structured data with template-specific fields. For Affiliate Agreements: exclusivity, non-compete, revenue sharing, minimum commitments. For Co-Branding: IP ownership, license grants, brand usage rights. Templates handle 70-80% of legal entity extraction with high precision; LLMs process the remaining 20-30% of non-standard provisions.

Batch processing optimization reduces API overhead through multi-stage extraction. The MuSEE Framework (2024, arxiv 2402.04437) decomposes extraction into stages processed in batches where all predictions within each stage are batched together, with each text encoded only once by the encoder. Parallel training with teacher forcing strategy significantly reduces generation time. For legal documents, configure batch_size based on document complexity: simpler contracts (employment agreements) support larger batches (30-50), complex M&A documents require smaller batches (10-20) for quality control. Your current batch_size=20 falls in the optimal range, but implementing staged extraction (entities first batch, relationships second batch) would reduce redundant encoding.

For your implementation: Implement a three-tier extraction system. **Tier 1** (rule-based): Extract dates, dollar amounts, standard citation formats, party names from signature blocks using regex and spaCy patterns. **Tier 2** (small model routing): Use GPT-4o-mini or fine-tuned Llama-2-7b for straightforward entity and relationship extraction (60-70% of extractions). **Tier 3** (premium LLM): Route ambiguous entities, complex cross-document relationships, and novel clause types to GPT-4 or Claude. Implement a simple classifier (BERT-based, 300M parameters) trained on 500-1000 labeled examples to route between tiers. Expected savings: 60-75% reduction in LLM API costs with maintained or improved accuracy.

## Alternatives to per-chunk LLM calls reduce redundant processing

Your current per-chunk extraction approach with separate entity and relationship calls creates redundancy opportunities. **Document-level extraction** with refinement proves more efficient. The KGGen approach (arxiv 2502.09956v1, 2025) employs multi-stage extraction: (1) entity/relation extraction from full document or large sections, (2) aggregation across sources, (3) iterative clustering to resolve duplicates. This processes entire documents once then refines, outperforming OpenIE and GraphRAG on the MINE benchmark. Integration with DSPy ensures consistent JSON-formatted outputs, eliminating post-processing overhead.

**Hierarchical extraction** using the LightKGG Framework (arxiv 2510.23341, 2024) demonstrates that Small Language Models (SLMs) with task-specific architectures compete with GPT-4o while being dramatically cheaper. Testing Phi-3.5-mini, GLM-Edge-1.5B, and DeepSeek-R1-Distill-Qwen-1.5B on a single NVIDIA RTX4090 GPU (24GB VRAM) achieved competitive performance for knowledge graph generation. For legal documents, this means deploying smaller models for initial extraction passes, then using larger models only for validation and disambiguation.

**Sliding window with overlap** addresses context fragmentation in chunk-based processing. Neo4j's configurable implementation uses 500-word chunks with 50-100 word overlap as typical configuration. For legal documents with complex cross-references, increase overlap to 20% of chunk size (your 200-char overlap on potentially 1000-char chunks is reasonable). However, the more efficient approach combines sliding windows with **graph-based context propagation**: build a document graph connecting entities across chunks, propagate information through graph edges, enabling reasoning over long-distance dependencies without redundant LLM calls for overlapping content.

The **query-based summarization approach** (2024) addresses token limitations for lengthy contracts through two steps: (1) BM25 summarization extracting sentences pertinent to predefined queries, (2) GPT-3.5 for precise information extraction from reduced text. This maintains core information while addressing token limits. For 200+ page legal documents, implement query-focused extraction: instead of processing all chunks for all entity types, first identify relevant sections for each entity type (parties appear in preamble and signature blocks, financial terms in payment sections, termination clauses in specific sections), then apply targeted extraction.

For your workflow optimization: Replace separate per-chunk entity and relationship calls with **joint extraction** using span-based generation. The Autoregressive Text-to-Graph Framework (ATG, AAAI 2024) generates nodes (text spans) and edges (relation triplets) simultaneously using transformer encoder-decoder with pointing mechanism, eliminating redundant text encoding. Implement document-level entity resolution before relationship extraction to avoid extracting relationships for duplicate entity mentions. Use hierarchical chunking (LangChain RecursiveCharacterTextSplitter): Stage 1 splits on document structure (sections, articles, clauses), Stage 2 applies character-level splitting when sections exceed limits (2048 chars recommended, 200-char overlap validated in production). This reduces total LLM calls by 30-40% compared to uniform per-chunk processing.

## Cross-document entity resolution requires sophisticated embedding and graph methods

Cross-document entity resolution becomes critical when processing multiple legal documents for the same matter (multiple contracts between parties, regulatory filings, amendments) or building enterprise-wide legal knowledge graphs. **Entity deduplication and canonicalization** prevents graph pollution where "Acme Inc.", "Acme Incorporated", and "Acme Corp." create separate nodes for the same entity.

The state-of-the-art approach combines **embedding-based similarity with graph-based validation**. Ahmed et al.'s Cross-Document Contextual Coreference Resolution (2024, arXiv 2504.05767v1) uses contextual embeddings (BERT-based) to capture semantic relationships between entity mentions, then employs graph-based inference techniques to resolve coreferences across documents. Performance metrics: Llama-3 on CoNLL-2012 achieved F1: 73.9%, ThaiCoref reached F1: 78.7%, GPT-3.5 on complex queries achieved F1: 72.5%. The method addresses cross-document event coreference resolution challenges by integrating knowledge graph structure with textual mentions.

**Embedding-based entity matching** uses domain-specific models. For legal documents, Legal-BGE-base (fine-tuned BGE embeddings) and Legal-E5 (E5 model adapted for legal text) outperform general-purpose embeddings. Similarity thresholds from 2024 industry practice: cosine similarity ≥0.85-0.90 for high-confidence automatic matches deployed without review, 0.70-0.85 for medium-confidence requiring human review, <0.70 for likely non-matches. Context-specific adjustments: structured legal text (statutes) uses higher thresholds (0.90+), unstructured case law accepts lower thresholds (0.70-0.80). The EM-Join framework (2025, SciTePress) achieves superior performance on 3 of 13 datasets while being 3x faster than Ditto baseline, using threshold selection heuristics (0.5 default, 0.8-0.9 for high precision).

**Graph-based entity alignment** methods prove particularly effective for legal knowledge graphs. Jiang et al.'s Simple-HHEA (WWW 2024) adaptively integrates entity name, structure, and temporal information, achieving 79.4% F1 on Chinese-English alignment (19.9% improvement over baselines). Key finding: GNN-based methods struggle with highly heterogeneous graphs common in legal domains. **Name similarity often proves more important than graph structure** for legal entities, contrasting with social network or citation network entity resolution. Graph Attention Networks (GATs) combined with BERT encoding allocate weights among entities to enhance target entity identification, significantly outperforming baseline models in legal document knowledge extraction.

**Decision framework for when cross-document linking is necessary**: Apply when (1) merging company records from different jurisdictions, (2) building citation networks and precedent graphs, (3) conducting compliance checking across regulations, (4) performing M&A due diligence across thousands of contracts, or (5) constructing entity-centric legal research systems. Skip cross-document linking for (1) within-document analysis only, (2) pre-resolved datasets with unique IDs, (3) lightweight applications like keyword search, (4) small document sets (<100 documents) where manual review is faster. Cost-benefit analysis: average time saved per resolved entity is 2-5 minutes, cost of false positive is 10-30 minutes to correct, break-even threshold requires Precision ≥80%, Recall ≥70%.

**Scalability solutions** for large legal document collections include blocking and indexing (token blocking groups by shared n-grams, LSH for sub-linear approximate matching, HNSW for fast approximate nearest neighbor search), distributed processing (Apache Spark for large-scale pipelines, Dask for Python-based workflows), incremental resolution (streaming entity resolution processes new documents without full recomputation, cached embeddings for existing documents), and hierarchical approaches (NMFk hierarchical decomposition processed 28,251 statute sections into 985 clusters, coarse-to-fine matching first at category level then within categories). Real-world example: New Mexico Legal Corpus (2025) processed 265 constitutional sections, 28,251 statutes, 15,799 cases into 190,283 nodes with 16.9M edges, achieving <1 second query time for complex multi-hop queries.

For your implementation using (type, normalized_value) deduplication: Enhance with embedding-based similarity. After initial deduplication, compute embeddings for all entities within each type using Legal-BGE-base, perform hierarchical clustering with 0.85 threshold, and merge clusters while preserving provenance. Implement cross-chunk entity resolution before relationship extraction to ensure "Party A" mentioned in chunk 5 resolves to the same entity as "Purchaser" mentioned in chunk 47. For cross-document scenarios, deploy the three-tier resolution strategy: Tier 1 exact matching (string equality, normalized forms), Tier 2 high-confidence matching (embedding similarity ≥0.90, automated), Tier 3 medium-confidence (0.70 ≤ similarity <0.90, human-in-loop). Store canonical entity IDs in Neo4j with properties capturing all name variants and source document references.

## Joint extraction outperforms pipelines while GNNs enhance relationship prediction

**Joint entity-relationship extraction** eliminates error propagation inherent in pipeline approaches where mistakes in entity recognition cascade to relation extraction. The decomposition strategy with pointer mechanism (Li et al., Scientific Reports 2024) uses BERT encoding for semantic feature extraction with hierarchical modeling: head entity recognition followed by tail entity and relation extraction. Quantitative improvements from 2024 studies show joint models achieve 5.1% improvement in entity recognition F1 and 8.0% improvement in relation extraction F1 compared to pipeline approaches. Joint models excel at overlapping relation scenarios (NYT-multi dataset) where multiple relationships exist for the same entity pair.

The **Autoregressive Text-to-Graph Framework** (ATG, AAAI 2024) represents current state-of-the-art for joint extraction, using span-based generation of linearized graphs with transformer encoder-decoder and pointing mechanism. This generates nodes (text spans) and edges (relation triplets) simultaneously, capturing structural characteristics and boundaries while grounding output in original text. The multi-head selection approach treats relation extraction as selecting multiple relations per entity using CRF layers for entity recognition, achieving better precision-recall balance by eliminating exhaustive entity pair enumeration. For your system, this means replacing separate entity then relationship LLM calls with single joint extraction prompts structured to output JSON with both entities and relationships simultaneously.

**Graph Neural Networks for relationship prediction** show strong performance in legal knowledge graphs. GATs (Graph Attention Networks) combined with BERT encoding allocate attention weights among entities to enhance target entity identification. Relational Graph Neural Networks (RGNNs) specialize in heterogeneous legal knowledge graphs handling multiple node types (cases, laws, people, organizations, citations) and edge types (cites, references, authored_by, filed_in). TensorFlow GNN 1.0 (released 2024) provides production-ready framework with dynamic subgraph sampling for scalability to millions of entities and message-passing mechanisms with heterogeneous edge types.

Graph LSTM frameworks extend to cross-sentence n-ary relation extraction, incorporating intra-sentential and inter-sentential dependencies through sequential, syntactic, and discourse relations. BERT-GT (Graph Transformer) uses neighbor-attention mechanism calculating attention only among neighbor tokens, critical for long texts and cross-sentence tasks. Results show 5.44% accuracy improvement and 3.89% F1 improvement over state-of-the-art for cross-sentence relation extraction. For legal documents where relationships often span multiple sentences or clauses, this architecture proves essential.

**Template-based relationship extraction** achieves high precision for standard legal provisions. The LlamaExtract + Neo4j pipeline demonstrates practical implementation with contract-specific templates defining expected relationship types. For Affiliate Agreements: PARTY-has-EXCLUSIVITY-TERM, PARTY-bound-by-NON_COMPETE, PARTIES-split-REVENUE_SHARE, PARTY-commits-MINIMUM_VOLUME. Templates achieve 97.71% accuracy on DORA compliance extraction (35 information pieces per template across 5 template variations). However, templates fail with non-standard documents, requiring hybrid approaches combining template matching with LLM-based extraction for novel relationship types.

**Confidence scoring** for extracted relationships employs multiple techniques. Self-consistency sampling (generating 5-10 outputs with different sampling parameters, selecting majority answer) achieves 27.3% mean calibration error versus 42% for verbal confidence methods, proving superior for 11 of 13 benchmark tasks. The CCPS approach (2025) applies adversarial perturbations to final hidden states, training lightweight classifiers to predict answer correctness. Results across 8B-32B parameter models: 55% reduction in Expected Calibration Error, 21% reduction in Brier score, 5 percentage point accuracy increase, 4 percentage point AUCPR increase. Post-hoc calibration using isotonic regression reduces calibration error by 46% versus uncalibrated scores.

**Cross-chunk relationship extraction** requires specialized handling. Query-based summarization addresses token limitations by extracting sentences pertinent to relationship queries using BM25, then applying GPT-3.5 for precise relationship extraction from reduced text. Graph-based context propagation builds document graphs connecting entities across chunks, propagating information through graph edges to enable reasoning over long-distance dependencies. Entity-level graphs (EDCRE, CIKM 2020) focus on entity links (dependency edges between entities) and discourse relations between sentences, outperforming state-of-the-art on PubMed and DocRED datasets.

For your implementation: Transition from separate entity and relationship extraction to joint extraction using structured output prompts. Example prompt structure: "Extract all entities and their relationships from the following legal text. Output as JSON: {entities: [{id, type, text, span}], relationships: [{source_id, target_id, relation_type, evidence_text}]}". For cross-chunk relationships in 200+ page documents, implement two-pass extraction: Pass 1 extracts within-chunk entities and relationships, Pass 2 uses graph traversal to identify entity pairs appearing in different chunks without direct relationships, then prompts LLM with context from both chunks to extract potential cross-chunk relationships. Set confidence thresholds: auto-accept relationships with confidence ≥0.80, human review for 0.60-0.80, reject <0.60. Use self-consistency (5 samples, majority voting) for high-stakes relationships like liability caps, indemnification, and termination rights.

## Few-shot prompting and fine-tuning dramatically improve legal entity extraction accuracy

**Few-shot learning** enables legal entity type recognition with minimal labeled data, critical given annotation costs requiring specialized legal expertise. RAG few-shot approaches achieve F1 scores of 0.10-0.11 with GPT-4, Mistral 7B, and Llama 3. Incorporating even a single relevant example can improve performance 2-3x (Polat et al., 2025). The key insight: **relevant examples within prompts prove more beneficial than random or canonical examples**. Use retrieval mechanisms (Maximal Marginal Relevance) to select diverse, relevant examples for each extraction task. Optimal configuration: 1-3 examples per entity type in prompts, with diminishing returns beyond this. For legal entities, include examples covering edge cases like nested legislative references, multi-word party names with abbreviations, and complex citation formats.

**Parameter-efficient fine-tuning** provides cost-effective domain adaptation. LoRA (Low-Rank Adaptation) introduces low-rank matrices (rank=16) in frozen transformer layers, reducing trainable parameters by 99.9% while achieving 95-98% of full fine-tuning performance. Memory benefits: ~3x lower usage, 70% reduction in requirements, enabling fine-tuning on consumer GPUs (24GB VRAM for 13B+ models). Applied to SaulLM-7B (legal-domain-specific model pre-trained on 19M legal documents), LoRA fine-tuning on CLAUDETTE-ToS benchmark for unfair clause detection achieved within 1-2% F1 score of full fine-tuning. Cost analysis: 70-80% cost reduction versus full fine-tuning makes legal AI accessible to smaller firms.

QLoRA (Quantized LoRA) combines LoRA with 4-bit quantization, enabling 4x memory reduction compared to LoRA. Base model weights stored in 4-bit, LoRA adapters in 16-bit, using double quantization for further compression. Successfully fine-tuned LLaMA-3B/7B and TinyLlama-1.1B for legal tasks with competitive recall. Reduces training costs by 75-80% compared to full fine-tuning, enabling 13B+ model fine-tuning on single 24GB GPUs. Trade-off: slightly slower than LoRA due to quantization/dequantization steps and some precision loss for highest-accuracy applications.

**Post-processing and validation** techniques improve extraction quality by 3-6% F1 score. Rule-based post-processing completes partial entity tags in multi-word legal terms, fixes unbalanced parentheses in isolated entities, normalizes date formats and legal citations, and applies domain-specific rules for entity boundaries. Citation validation verifies legal citations follow proper format (e.g., "22 C.F.R. § 40.51"), statute reference checking validates references point to actual legislative provisions, and abbreviation resolution expands legal abbreviations using tools like BioC abbreviation resolver. Ensemble methods combining outputs from multiple classifiers (ME, CRF, SVM) with weighted voting achieve F1 scores of 89.73% for legal NER.

**Human-in-the-loop strategies** optimize annotation efficiency. Uncertainty sampling selects examples where models are least confident for human annotation, focusing on boundary cases and ambiguous legal entities. The "tag-a-little-learn-a-little" approach pre-tags documents with current model, humans correct errors in incremental batches, then retrain frequently with corrected examples, reducing annotation burden by 40-60%. Implement escalation thresholds: route extractions with model confidence <0.70 to human review, auto-accept ≥0.90, batch review 0.70-0.90 periodically. For high-precision requirements (>95% accuracy), HITL proves essential for novel entity types and edge cases.

**Self-consistency and ensemble methods** enhance reliability. Universal Self-Consistency generates multiple completions, then uses the LLM itself to select the most consistent response based on majority consensus, handling free-form answers without fixed answer sets. This matches or exceeds standard self-consistency while reducing hallucinations in legal content. For your system, implement self-consistency for critical entity types (parties, financial terms, termination conditions): generate 5 responses with temperature=0.7, extract entities from each, select entities appearing in ≥3 of 5 responses. Cost: 5x inference, but reserve for high-value extractions where errors are expensive.

**Legal benchmark datasets** enable rigorous evaluation. LegalBench (2023) provides 162 tasks from 40 legal professional contributors covering hearsay detection, definition extraction, rule QA, and clause classification. CUAD (Contract Understanding Atticus Dataset) offers 13,000+ expert annotations by 40+ lawyers across 9,283 pages reviewed 4+ times. German Legal Entity Recognition dataset contains 67,000 sentences with 54,000 annotated entities across 19 fine-grained classes including judges, lawyers, courts, laws, ordinances, contracts, and decisions. InLegalNER covers Indian court judgments with 46,545 annotated entities across 14 types. For validation, test your extraction system on CUAD's contract clause extraction tasks and LegalBench's entity recognition benchmarks to compare against published baselines.

**Prompt engineering best practices** for legal entities include clear task definitions ("Extract all contracting parties including their roles: buyer, seller, guarantor, agent"), explicit schema definitions with JSON structure, inclusion of legal context in entity definitions ("Legal citations follow format: Volume Reporter Page, e.g., 347 U.S. 483"), and chain-of-thought prompting for complex entities ("First identify the contract type, then extract parties appropriate to that contract type"). ChatExtract method (Nature Communications 2024) using conversational LLM with follow-up questions achieves ~90% precision and recall through purposeful redundancy and uncertainty introduction.

For your implementation: Start with few-shot prompting using 3 manually crafted examples per entity type (parties, dates, financial terms, obligations, termination conditions, indemnification clauses). Implement retrieval-based example selection: for each document chunk, retrieve 3 most similar previously annotated examples using Legal-BGE embeddings, inject into prompt. After processing 500-1000 documents with periodic human review, fine-tune a Llama-2-7b model using QLoRA on corrected extractions. Expected improvements: 15-25% F1 score increase over zero-shot GPT-4o-mini, 3-5% additional improvement from fine-tuning. Deploy post-processing rules for citation normalization and multi-word entity boundary correction. Implement HITL with uncertainty sampling: extract entities with confidence scores, route lowest 10% confidence to human review weekly, retrain monthly with corrected examples.

## Incremental graph construction with Neo4j provides optimal performance and cost

**Architecture patterns** fundamentally determine system scalability and cost-efficiency. Incremental knowledge graph construction dramatically outperforms full reprocessing for legal document workflows involving frequent updates (contract amendments, regulatory changes, new filings). IncRML framework (Semantic Web Journal, 2024) demonstrated **up to 315.83x less storage, 4.59x less CPU time, and 1.51x less memory** compared to full reprocessing, with KG construction time reduced by up to 4.41x for larger datasets. The architecture supports snapshot-based and timestamp-based change data capture, using RML + FnO functions for detecting and classifying changes across mapping processes.

The **iText2KG approach** (September 2024, arXiv:2409.03284) provides zero-shot, topic-independent incremental construction using four modules: (1) Document Distiller reformulating raw documents into semantic blocks, (2) Incremental Entity Extractor avoiding semantic duplicates, (3) Incremental Relations Extractor, and (4) Graph Integrator with Neo4j visualization. Key innovation: entities and relations are semantically unique without post-processing. For legal documents requiring change tracking at section/clause level with version history for audit trails, incremental construction proves essential.

**Graph storage selection** critically impacts performance. Neo4j remains the production standard, recognized by Forrester and Gartner, with 60% faster performance than MySQL for simple queries and 180x faster for 3-hop queries. 2024 improvements include new block storage format (breakthrough performance), Parallel Runtime for large analytical queries, Call in Transactions for parallel writes, and native vector index support (20-50ms for top-50 similarity from 1M vectors at 960 dimensions). Cypher query language optimizes for relationship traversal essential in legal reasoning. Strong ACID compliance meets regulatory requirements. For massive legal corpora exceeding 100M+ documents requiring horizontal scaling, NebulaGraph significantly outperforms Neo4j for 1-hop, 2-hop, and shared-friends queries at scale. Memgraph offers up to 120x faster performance on specific queries with in-memory C++ architecture using 1/4 of Neo4j's memory, optimal for real-time processing and low-latency requirements.

**Chunk size optimization** balances context preservation with processing efficiency. Research-backed recommendations for legal documents: **2048 characters with 200-character overlap** (10%) validated in production GraphRAG systems. Hierarchical chunking using LangChain RecursiveCharacterTextSplitter splits first on Markdown headers preserving semantic cohesion, then character-level splitting when sections exceed limits. For legal documents, respect natural divisions (sections, subsections, clauses) through structure-aware splitting. Legal sentences average 68 words; nested entities and legislative cross-references complicate chunking. Recommendations: sentence-level for entity extraction (preserves grammatical context), paragraph-level (500-1000 tokens) for summarization, multi-sentence windows (3-5 sentences) with overlap for relation extraction, clause-level (variable 100-500 tokens) for contract analysis. Empirical finding: if chunks make sense to humans without context, they work for LLMs.

**Parallel processing patterns** for 200+ page legal documents require controlled concurrency. Neo4j cannot handle >20 concurrent requests without data corruption risk; use work queues (Kafka, RabbitMQ) with 10-15 workers. The GraphRAG 2024 paper (arXiv:2507.03226) validates sentence-level segmentation with sliding window batching (batch size=3 sentences, overlap=1 sentence) and parallel API calls for entity/relation extraction. For your ThreadPoolExecutor with max_workers=10, this falls within safe concurrency limits. Optimize batch processing through multi-stage extraction pipelines where all predictions within each stage are batched together with each text encoded only once.

**Vector database integration** enables hybrid GraphRAG architectures showing 15% improvement over traditional RAG. Unified storage using Neo4j's native vector index (introduced 2024) eliminates synchronization overhead between separate graph and vector databases. Integration pattern: Stage 1 vector search identifies seed entities using BM25 + dense embeddings, Stage 2 graph traversal from seed nodes (1-2 hops), Stage 3 neural reranking with cosine similarity, Stage 4 fusion using Reciprocal Rank Fusion combining vector and graph results. For legal contract Q&A: embed user query, vector search retrieves top-K clauses/sections, extract entity mentions, graph traversal identifies related obligations and cross-references, LLM summarization with retrieved context.

**Streaming versus batch processing** depends on use case. Batch processing suits quarterly compliance audits, annual contract reviews, and bulk document imports (cost-effective, easier debugging, quality control checkpoints). Streaming processing enables real-time contract monitoring, live regulatory updates, and fraud detection (sub-millisecond detection, timely insights critical for legal deadlines). Hybrid approach recommended: batch processing for historical document initial load, incremental streaming for new filings and amendments. IncRML supports both batch and streaming via LDES protocol. Quine Streaming Graph processes millions of complex multi-hop events per second with sub-millisecond latency, applicable to real-time detection of conflicting clauses across streaming contracts.

For your architecture optimization: Implement incremental graph construction using IncRML patterns. Deploy change detection at document version level: when contracts are amended, extract only changed sections and update affected graph nodes/edges. Use Neo4j with native vector index for unified storage eliminating separate vector database. Configure hierarchical chunking: detect document structure (articles, sections, clauses) using ML-based section detection, split at natural boundaries with 2048-char maximum and 200-char overlap fallback for large sections. Maintain your ThreadPoolExecutor with max_workers=10 but implement work queue (RabbitMQ) for better load distribution and failure recovery. Add monitoring for Neo4j query latency (target p99 <1s), memory usage, and GC pauses. Implement Graph Integrator similar to iText2KG: after each batch processing, resolve entity duplicates using embedding similarity (Legal-BGE-base, threshold=0.85), merge nodes while preserving provenance, and update relationship targets to canonical entity IDs.

## Practical recommendations for 50-80% cost reduction in your implementation

Your current system using gpt-4o-mini with parallel processing provides a solid foundation. To achieve 50-80% cost reduction while improving accuracy, implement these optimizations in priority order based on expected impact:

**Phase 1: Immediate optimizations (30-40% cost reduction, 2-4 weeks)**

Deploy prompt compression using LLMLingua-2 on all entity and relationship extraction prompts. Implementation: Install microsoft/LLMLingua, configure 10-15x compression ratio (conservative, minimal quality loss), apply to prompts before GPT-4o-mini calls. Expected savings: 20-30% token reduction. Add semantic caching with Redis storing embeddings for entity type queries (Legal-BGE-base embeddings, cosine similarity threshold 0.85). Cache frequent patterns: standard clause types (force majeure, indemnification, governing law), common entity structures. Expected savings: 15-20% reduction in redundant LLM calls for legal contracts with standardized provisions. Implement KV caching for long document processing and response caching for identical chunks across documents. Combined caching expected savings: 25-35%.

**Phase 2: Hybrid extraction architecture (additional 20-30% reduction, 4-6 weeks)**

Build three-tier extraction system. Tier 1 rule-based extraction: Deploy spaCy with custom patterns for dates (various legal formats), dollar amounts, standard citations (use legal citation parser), party names from signature blocks and preamble. Expected coverage: 30-40% of entities with near-perfect accuracy, essentially free computation. Tier 2 model routing: Train DeBERTa-v3-base (300M parameters) classifier on 500 annotated chunks labeled as "simple" or "complex" extraction. Route simple extractions to GPT-4o-mini, complex to GPT-4 or use GPT-4o-mini with enhanced prompting. Expected savings: 40% reduction in expensive model calls. Tier 3 template matching: Build templates for 5-10 common contract types in your corpus (employment agreements, NDAs, service contracts, lease agreements). Use LlamaExtract pattern: classify document type, apply type-specific extraction schema, fall back to LLM for non-matching sections. Expected coverage: 60-70% of standardized contracts, 15-25% additional cost reduction.

**Phase 3: Advanced optimization (additional 10-20% reduction, 6-8 weeks)**

Implement joint entity-relationship extraction replacing separate calls. Use structured output prompts: "Extract all entities (parties, dates, financial terms, obligations) and relationships between them. Output JSON: {entities: [...], relationships: [...]}". Single LLM call per chunk instead of two sequential calls reduces encoding overhead by ~40%. Configure with self-consistency for critical relationships: generate 3 outputs with temperature=0.7 for high-stakes extractions (termination rights, liability caps, exclusivity), use majority voting. Reserve for 10-20% of highest-value extractions where errors are expensive. Deploy hierarchical chunking with structure detection: Use ML model or rule-based approach to detect document structure (sections, articles, clauses based on numbering patterns and indentation), split at natural boundaries preserving semantic units, apply 2048-char/200-char overlap only when sections exceed limits. Reduces context fragmentation and cross-chunk relationship extraction complexity.

**Phase 4: Incremental graph construction (operational efficiency, 8-12 weeks)**

Implement incremental updates using IncRML patterns. Track document versions, extract only changed sections on contract amendments, use Neo4j MERGE operations for upsert semantics updating existing nodes/edges. Deploy entity resolution enhancement: After per-chunk extraction, compute Legal-BGE-base embeddings for all extracted entities within document, perform agglomerative clustering with distance threshold=0.15 (corresponds to cosine similarity 0.85), merge entity clusters assigning canonical IDs, update all relationships to reference canonical entities. Implement cross-chunk relationship extraction optimization: Two-pass approach where Pass 1 extracts within-chunk relationships, Pass 2 identifies entity co-occurrences across chunks using sliding window, queries LLM only for entity pairs without established relationships providing context from both chunks. Reduces cross-chunk LLM calls by 60-70%.

**Phase 5: Quality improvements and fine-tuning (accuracy enhancement, ongoing)**

Create legal entity benchmark from 100-200 manually annotated documents covering your contract types. Evaluate current system establishing baselines (entity extraction F1, relationship extraction F1, end-to-end accuracy). Implement few-shot with retrieval: For each extraction, retrieve 3 most similar annotated examples using Legal-BGE embeddings, inject into prompt. Expected improvement: 10-20% F1 increase. After 500-1000 processed documents with periodic review, fine-tune Llama-2-7b using QLoRA on your specific legal entity types and contract structures. Expected improvement: Additional 5-10% F1 increase, enables replacing some GPT-4o-mini calls with fine-tuned smaller model. Deploy HITL with active learning: Score all extractions by confidence, route lowest 5-10% to weekly human review, incorporate corrections into training set, retrain monthly. Continuous accuracy improvement while building proprietary training data.

**Monitoring and validation**

Implement comprehensive logging: track per-document costs (tokens consumed, LLM calls, processing time), entity extraction metrics (precision, recall, F1 per entity type), relationship extraction metrics, end-to-end processing latency, cache hit rates (semantic cache, KV cache, response cache). Set up alerting for regressions: accuracy drops >5%, cost spikes >20%, latency increases >2x. Run A/B tests for each optimization phase: process 100-document sample with old approach and new approach in parallel, compare costs and accuracy, validate improvements before full deployment. Maintain test set of 50-100 challenging documents as regression suite, evaluate after each change.

**Expected cumulative impact**

Implementing Phases 1-4 achieves 60-80% cost reduction: Prompt compression (25%), caching (25%), hybrid architecture (25%), joint extraction and optimization (15%) compound to 70-75% total reduction. Quality improvements in Phase 5 increase accuracy by 15-25% F1 while enabling further cost reductions through model substitution. Your system processing 200+ page documents would see particularly dramatic improvements: document-level deduplication and cross-chunk optimization especially benefit long documents, incremental updates amortize costs across contract lifecycle (original + amendments), template matching captures standardized sections in long agreements.

This roadmap delivers 50% cost reduction after Phase 1 (1 month), 70% reduction after Phase 3 (3 months), and 75-80% reduction after Phase 4 (6 months) while simultaneously improving extraction accuracy by 15-25% through better entity resolution, joint extraction reducing error propagation, and domain-specific fine-tuning. The approach maintains your parallel processing architecture while optimizing what work is done in parallel, preserves your entity deduplication concept while enhancing it with embeddings, and builds incrementally allowing validation at each phase before proceeding.
# Jak spustit RAG Pipeline

## ğŸš€ RychlÃ½ Start

### 1. Nastav API klÃ­Ä v `.env`

```bash
# OtevÅ™i .env soubor
nano .env
```

```bash
# MinimÃ¡lnÃ­ konfigurace (Claude + BGE-M3-v2 LOCAL):
ANTHROPIC_API_KEY=sk-ant-...    # Pro LLM (summaries)

# BGE-M3-v2 bÄ›Å¾Ã­ LOKÃLNÄš na tvÃ©m M1 Macu - Å¾Ã¡dnÃ½ API klÃ­Ä nepotÅ™ebuje! ğŸš€
```

**ZÃ­skÃ¡nÃ­ API klÃ­Äe:**
- Claude: https://console.anthropic.com/

**ProÄ BGE-M3-v2?**
- âœ… BÄ›Å¾Ã­ lokÃ¡lnÄ› na M1 (MPS acceleration)
- âœ… Å½Ã¡dnÃ© API klÃ­Äe, Å¾Ã¡dnÃ© nÃ¡klady
- âœ… Multilingual (100+ jazykÅ¯ vÄetnÄ› ÄeÅ¡tiny)
- âœ… SOTA performance (close to commercial APIs)

### 2. SpusÅ¥ pipeline

```bash
# ZÃ¡kladnÃ­ pouÅ¾itÃ­
python run_pipeline.py <cesta_k_pdf>

# PÅ™Ã­klad
python run_pipeline.py "data/regulace/GRI/GRI 306_ Effluents and Waste 2016.pdf"
```

### 3. Najdi vÃ½stupy

VÅ¡echny vÃ½stupy jsou v:
```
output/<nÃ¡zev_dokumentu>/<timestamp>/
â”œâ”€â”€ phase1_extraction.json      # Struktura dokumentu
â”œâ”€â”€ phase2_summaries.json       # GenerovanÃ© summaries
â”œâ”€â”€ phase3_chunks.json          # Multi-layer chunky
â””â”€â”€ phase4_vector_store/        # FAISS indexy
    â”œâ”€â”€ layer1.index
    â”œâ”€â”€ layer2.index
    â”œâ”€â”€ layer3.index
    â””â”€â”€ metadata.pkl
```

---

## âš™ï¸ Konfigurace ModelÅ¯

### VÃ½bÄ›r LLM (pro summaries)

Edituj `.env`:

```bash
# Claude Sonnet 4.5 (default, doporuÄeno)
LLM_PROVIDER=claude
LLM_MODEL=claude-sonnet-4-5-20250929
ANTHROPIC_API_KEY=sk-ant-...

# Claude Haiku 4.5 (rychlejÅ¡Ã­, levnÄ›jÅ¡Ã­)
LLM_PROVIDER=claude
LLM_MODEL=claude-haiku-4-5-20251001
ANTHROPIC_API_KEY=sk-ant-...

# OpenAI GPT-4o Mini
LLM_PROVIDER=openai
LLM_MODEL=gpt-4o-mini
OPENAI_API_KEY=sk-...
```

### VÃ½bÄ›r Embedding Modelu

```bash
# BGE-M3-v2 (DEFAULT, doporuÄeno pro M1 Mac) â­
EMBEDDING_PROVIDER=huggingface
EMBEDDING_MODEL=bge-m3
# Å½Ã¡dnÃ½ API klÃ­Ä - bÄ›Å¾Ã­ lokÃ¡lnÄ› na M1 s MPS acceleration!
# Features: 1024D, multilingual (Czech), 8192 tokens, ZDARMA

# === Alternativy (vyÅ¾adujÃ­ API klÃ­Äe) ===

# Kanon 2 (#1 na MLEB 2025)
EMBEDDING_PROVIDER=voyage
EMBEDDING_MODEL=kanon-2
VOYAGE_API_KEY=pa-...

# Voyage 3 Large (#2 na MLEB)
EMBEDDING_PROVIDER=voyage
EMBEDDING_MODEL=voyage-3-large
VOYAGE_API_KEY=pa-...

# Voyage Law 2 (legal-optimized)
EMBEDDING_PROVIDER=voyage
EMBEDDING_MODEL=voyage-law-2
VOYAGE_API_KEY=pa-...

# OpenAI text-embedding-3-large
EMBEDDING_PROVIDER=openai
EMBEDDING_MODEL=text-embedding-3-large
OPENAI_API_KEY=sk-...
```

---

## ğŸ“Š VÃ½stup Pipeline

### PHASE 1: Extraction (phase1_extraction.json)

```json
{
  "document_id": "GRI_306_2016",
  "num_sections": 118,
  "hierarchy_depth": 4,
  "sections": [
    {
      "section_id": "sec_001",
      "title": "Introduction",
      "level": 1,
      "depth": 0,
      "path": "Introduction",
      "page_number": 1
    }
  ]
}
```

### PHASE 2: Summaries (phase2_summaries.json)

```json
{
  "document_id": "GRI_306_2016",
  "document_summary": "Technical specification for waste management...",
  "section_summaries": [
    {
      "section_id": "sec_001",
      "title": "Introduction",
      "summary": "Overview of waste management procedures..."
    }
  ]
}
```

### PHASE 3: Chunks (phase3_chunks.json)

```json
{
  "chunking_stats": {
    "layer1_count": 1,
    "layer2_count": 118,
    "layer3_count": 123,
    "total_chunks": 242,
    "layer3_avg_size": 450,
    "sac_avg_overhead": 150
  },
  "layer3": [
    {
      "chunk_id": "GRI_306_2016_L3_sec_001_chunk_0",
      "content": "[DOC SUMMARY] ... [CHUNK] The primary...",
      "raw_content": "The primary cooling system...",
      "metadata": {
        "layer": 3,
        "section_title": "Introduction",
        "page_number": 1
      }
    }
  ]
}
```

### PHASE 4: Vector Store (phase4_vector_store/)

```
phase4_vector_store/
â”œâ”€â”€ layer1.index        # FAISS index pro Layer 1 (1 vektor)
â”œâ”€â”€ layer2.index        # FAISS index pro Layer 2 (118 vektorÅ¯)
â”œâ”€â”€ layer3.index        # FAISS index pro Layer 3 (123 vektorÅ¯)
â””â”€â”€ metadata.pkl        # Metadata pro vÅ¡echny chunky
```

---

## ğŸ§ª TestovÃ¡nÃ­

### Test na GRI 306 dokumentu

```bash
# StÃ¡hni testovacÃ­ dokument
# UÅ¾ mÃ¡Å¡ v: data/regulace/GRI/GRI 306_ Effluents and Waste 2016.pdf

# SpusÅ¥ pipeline
python run_pipeline.py "data/regulace/GRI/GRI 306_ Effluents and Waste 2016.pdf"

# OÄekÃ¡vanÃ© vÃ½sledky:
# - 118 sekcÃ­
# - Hloubka hierarchie: 4
# - 242 celkovÃ½ch chunkÅ¯ (1 + 118 + 123)
# - ~2.9 MB vector store (pro Kanon 2: 1024D)
```

### Test s vlastnÃ­m dokumentem

```bash
# 1. VloÅ¾ PDF do data/
cp /path/to/your.pdf data/

# 2. SpusÅ¥ pipeline
python run_pipeline.py data/your.pdf

# 3. Najdi vÃ½stupy v:
ls -la output/your/<timestamp>/
```

---

## ğŸ” Kontrola VÃ½stupÅ¯

### ProhlÃ©dni JSON vÃ½stupy

```bash
# PHASE 1 - Struktura
cat output/*/phase1_extraction.json | jq '.sections[] | {title, level, depth}'

# PHASE 2 - Summaries
cat output/*/phase2_summaries.json | jq '.document_summary'

# PHASE 3 - Chunking stats
cat output/*/phase3_chunks.json | jq '.chunking_stats'

# PHASE 3 - PrvnÃ­ 3 chunky
cat output/*/phase3_chunks.json | jq '.layer3[0:3] | .[] | {chunk_id, section_title}'
```

### Zkontroluj Vector Store

```bash
# Velikost indexÅ¯
du -h output/*/phase4_vector_store/

# Struktura
ls -la output/*/phase4_vector_store/
```

---

## ğŸ’° NÃ¡klady

### Claude Sonnet 4.5 (summaries)
- **Vstup:** $3 / 1M tokens
- **VÃ½stup:** $15 / 1M tokens
- **GRI 306 (15 stran):** ~$0.02

### Claude Haiku 4.5 (summaries)
- **Vstup:** $0.80 / 1M tokens
- **VÃ½stup:** $4 / 1M tokens
- **GRI 306 (15 stran):** ~$0.005

### BGE-M3-v2 (embeddings - LOCAL)
- **Cena:** ZDARMA ğŸ‰
- **BÄ›Å¾Ã­ lokÃ¡lnÄ› na M1 Macu**
- **Å½Ã¡dnÃ© API volÃ¡nÃ­, Å¾Ã¡dnÃ© nÃ¡klady**

**Celkem pro GRI 306:**
- Claude Sonnet + BGE-M3-v2 (LOCAL): ~$0.02 â­
- Claude Haiku + BGE-M3-v2 (LOCAL): ~$0.005 â­

**Ãšspora vs cloud embeddings:**
- Vs Kanon 2: $0.003 saved per document
- Vs OpenAI: $0.002 saved per document
- Pro 1000 dokumentÅ¯: **$2-3 uÅ¡etÅ™eno!**

---

## âš ï¸ Troubleshooting

### "ANTHROPIC_API_KEY required"

```bash
# Nastav Claude API klÃ­Ä
export ANTHROPIC_API_KEY="sk-ant-..."
# nebo edituj .env
```

### "VOYAGE_API_KEY required"

```bash
# Nastav Voyage AI klÃ­Ä
export VOYAGE_API_KEY="pa-..."
# nebo edituj .env

# Nebo pÅ™epni na OpenAI embeddings:
EMBEDDING_PROVIDER=openai
EMBEDDING_MODEL=text-embedding-3-large
```

### "voyageai package required"

```bash
# Nainstaluj Voyage AI SDK
uv pip install voyageai
```

### "anthropic package required"

```bash
# Nainstaluj Anthropic SDK
uv pip install anthropic
```

### "File not found"

```bash
# Zkontroluj cestu k PDF
ls -la "data/regulace/GRI/GRI 306_ Effluents and Waste 2016.pdf"

# Pozor na mezery - escapuj je nebo pouÅ¾ij uvozovky
python run_pipeline.py "data/my file.pdf"
```

---

## ğŸ“– DalÅ¡Ã­ Informace

- **KompletnÃ­ dokumentace:** `README.md`
- **PHASE 1-2 detaily:** `IMPLEMENTATION_SUMMARY.md`
- **PHASE 3 detaily:** `PHASE3_COMPLETE.md`
- **PHASE 4 detaily:** `PHASE4_COMPLETE.md`
- **Research foundation:** `PIPELINE.md`

---

## ğŸ¯ Next Steps (PHASE 5-7)

Po dokonÄenÃ­ PHASE 1-4 mÅ¯Å¾eÅ¡ implementovat:

**PHASE 5: Query & Retrieval**
- Embedding queries
- Hierarchical search (K=6)
- DRM prevention

**PHASE 6: Context Assembly**
- Strip SAC summaries
- Concatenate chunks
- Add citations

**PHASE 7: Answer Generation**
- Claude/GPT-4 integration
- Citation formatting
- Answer validation

---

**AktuÃ¡lnÃ­ Status:** PHASE 1-4 COMPLETED âœ…
**Updated:** 2025-10-20
